from dotenv import load_dotenv
import os
import random
import time
import requests
import praw
import re
import sys
import time
import json
import os
import base64
import random
import hashlib
import requests
import subprocess
from pathlib import Path
from urllib.parse import unquote
from PIL import Image
import io
from bs4 import BeautifulSoup
import asyncio
import threading
from twscrape import API, gather
from types import SimpleNamespace
import shutil
import m3u8
import unicodedata
import logging
# Pnytter kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak
# Lazy import iÃ§in Google AI modÃ¼llerini kaldÄ±r - ihtiyaÃ§ duyulduÄŸunda import edilecek

# FastAPI for web service - lazy import
# from fastapi import FastAPI, Request - lazy import
# from fastapi.responses import PlainTextResponse - lazy import
# import uvicorn - lazy import
# import psycopg2 - lazy import
# from psycopg2.extras import RealDictCursor - lazy import

# Resolve script directory and accounts DB absolute path early
SCRIPT_DIR = Path(__file__).resolve().parent
_env_db = os.environ.get("ACCOUNTS_DB_PATH", "accounts.db")
if os.path.isabs(_env_db):
    ACCOUNTS_DB_PATH = _env_db
else:
    ACCOUNTS_DB_PATH = str((SCRIPT_DIR / _env_db).resolve())

# Ensure accounts.db exists (for Railway/containers without shell)
def _ensure_accounts_db():
    try:
        db_path = Path(ACCOUNTS_DB_PATH)
        # Ensure parent directory exists
        db_path.parent.mkdir(parents=True, exist_ok=True)
        if db_path.exists():
            # Quick sanity check for readability
            if not os.access(db_path, os.R_OK):
                print(f"[UYARI] accounts.db okunamÄ±yor (izinler): {db_path}")
            return
        b64 = os.environ.get("ACCOUNTS_DB_B64")
        if not b64:
            # Nothing to do; twscrape will log "No active accounts" later if required
            print("[INFO] ACCOUNTS_DB_B64 bulunamadÄ±; accounts.db oluÅŸturulmadÄ±")
            return
        try:
            data = base64.b64decode(b64)
        except Exception:
            # Handle potential newlines/spaces
            data = base64.b64decode(b64.encode("ascii"))
        db_path.write_bytes(data)
        try:
            # Best-effort chown in Linux containers; ignore on Windows
            import pwd, grp  # type: ignore
            import os as _os
            uid = _os.getuid() if hasattr(_os, "getuid") else None
            gid = _os.getgid() if hasattr(_os, "getgid") else None
            if uid is not None and gid is not None:
                _os.chown(str(db_path), uid, gid)
        except Exception:
            pass
        print(f"[+] accounts.db oluÅŸturuldu: {db_path}")
    except Exception as e:
        print(f"[UYARI] accounts.db oluÅŸturulamadÄ±: {e}")

_ensure_accounts_db()

# User-Agent rotation pool
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:132.0) Gecko/20100101 Firefox/132.0'
]

# Session pool iÃ§in global deÄŸiÅŸkenler
SESSION_POOL = {}
SESSION_LAST_USED = {}
MAX_SESSION_AGE = 1800  # 30 dakika

# Proxy ayarlarÄ±
PROXY_LIST = [
    # Ãœcretsiz HTTP proxy'ler - Ã§alÄ±ÅŸanlarÄ± ekleyin
    # {'http': 'http://proxy1:port', 'https': 'http://proxy1:port'},
    # {'http': 'http://proxy2:port', 'https': 'http://proxy2:port'},
]

# Tor proxy (eÄŸer kuruluysa)
TOR_PROXY = {
    'http': 'socks5://127.0.0.1:9050',
    'https': 'socks5://127.0.0.1:9050'
}

CURRENT_PROXY_INDEX = 0
USE_PROXY = os.getenv("USE_PROXY", "false").lower() == "true"
USE_TOR = os.getenv("USE_TOR", "false").lower() == "true"

# twscrape API instance
twscrape_api = None
from base64 import b64decode

# RedditWarp imports
import redditwarp.SYNC
from redditwarp.SYNC import Client as RedditWarpClient

# --- Localization helpers (Unicode + casing) ---
def _nfc(s: str) -> str:
    """Return NFC-normalized text to keep Turkish diacritics stable."""
    try:
        return unicodedata.normalize('NFC', s) if isinstance(s, str) else s
    except Exception:
        return s

def _is_all_caps_like(s: str) -> bool:
    """Heuristic: treat as ALL CAPS if >=80% of letters are uppercase and there is at least one letter."""
    letters = [ch for ch in s if ch.isalpha()]
    if not letters:
        return False
    upp = sum(1 for ch in letters if ch.isupper())
    return (upp / max(1, len(letters))) >= 0.8

def _deshout_en_sentence_case(s: str) -> str:
    """Convert shouty English text to sentence case to improve translation quality.
    Keeps non-letter characters as is. Simple sentence boundary heuristic."""
    try:
        s_low = s.lower()
        out = []
        make_upper = True
        for ch in s_low:
            out.append(ch.upper() if make_upper and ch.isalpha() else ch)
            if ch in '.!?\n':
                make_upper = True
            elif ch.isalpha():
                make_upper = False
        return ''.join(out)
    except Exception:
        return s

# --- Countdown detection helper ---
def _extract_countdown_days(text: str):
    """Detect patterns like '44 Days Until ...' in text and return the day count as int.
    Returns None if not found. Case-insensitive, tolerant of extra spaces and punctuation.
    """
    try:
        if not text:
            return None
        # Normalize whitespace
        t = ' '.join(str(text).split())
        # Common patterns: '44 Days Until', '10 day until', '3 DAYS UNTIL'
        m = re.search(r"\b(\d{1,3})\s*(day|days)\s+until\b", t, flags=re.IGNORECASE)
        if m:
            try:
                return int(m.group(1))
            except Exception:
                return None
        return None
    except Exception:
        return None

# Windows encoding sorununu gÃ¼venli ÅŸekilde Ã§Ã¶z (buffer olmayabilir)
if sys.platform.startswith('win'):
    import codecs
    try:
        stdout_base = getattr(sys.stdout, 'buffer', sys.stdout)
        stderr_base = getattr(sys.stderr, 'buffer', sys.stderr)
        sys.stdout = codecs.getwriter('utf-8')(stdout_base, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(stderr_base, 'strict')
    except Exception as _enc_e:
        print(f"[UYARI] Windows encoding ayarÄ± atlandÄ±: {_enc_e}")

load_dotenv()

SUBREDDIT = "bf6_tr"

REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
REDDIT_USERNAME = os.getenv("REDDIT_USERNAME")
REDDIT_PASSWORD = os.getenv("REDDIT_PASSWORD")
REDDIT_USER_AGENT = "python:bf6-gaming-news-bot:v1.1.0 (by /u/BFHaber_Bot)"
RAPIDAPI_TRANSLATE_KEY = os.getenv("RAPIDAPI_TRANSLATE_KEY")
DATABASE_URL = os.getenv("DATABASE_URL")

# Posted IDs storage backend selector
USE_DB_FOR_POSTED_IDS = bool(DATABASE_URL)
# If true, bot MUST have DB; otherwise exit instead of using file fallback
FAIL_IF_DB_UNAVAILABLE = os.getenv("FAIL_IF_DB_UNAVAILABLE", "true").lower() == "true"

# Retention and dedupe configuration
# How many posted tweet IDs to retain in storage (DB/file)
POSTED_IDS_RETENTION = int(os.getenv("POSTED_IDS_RETENTION", "200"))
# If enabled, skip any tweet with id <= last seen numeric id at startup/runtime
HIGH_WATERMARK_ENABLED = os.getenv("HIGH_WATERMARK_ENABLED", "true").lower() == "true"

# Translate cache (avoid repeated Gemini calls for identical input)
TRANSLATE_CACHE_MAX = int(os.getenv("TRANSLATE_CACHE_MAX", "500"))
try:
    from collections import OrderedDict
except Exception:
    OrderedDict = dict  # very rare fallback; order eviction won't be perfect
TRANSLATE_CACHE = OrderedDict()
TRANSLATE_CACHE_LOCK = threading.Lock()

# Belirli tweet ID'lerini asla Reddit'e gÃ¶ndermeyin (kullanÄ±cÄ± isteÄŸi)
# Bu ID'ler iÅŸlenmiÅŸ olarak da iÅŸaretlenir, bÃ¶ylece tekrar denenmez
EXCLUDED_TWEET_IDS = {
    "1958574822692462777",
    "19585522850022935535",
    "1958552050322616811",
    "1958551490953367669",
    "1958551346367324285",
    "1958548172915044642",
    "1958529168376770882",
    "1958287931153760384",
    "1958287931153760384",
    "1958025330372825431",
}

# Nitter konfigÃ¼rasyonu kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak
TWITTER_SCREENNAME = "TheBFWire"
# Twitter User ID (tercih edilen yÃ¶ntem - daha gÃ¼venilir)
TWITTER_USER_ID = os.getenv("TWITTER_USER_ID", "1939708158051500032").strip()
MIN_REQUEST_INTERVAL = 30  # Minimum seconds between any requests
LAST_REQUEST_TIME = 0  # Son istek zamanÄ±
TWSCRAPE_DETAIL_TIMEOUT = 8  # seconds to wait for tweet_details before skipping
REDDIT_MAX_VIDEO_SECONDS = int(os.getenv("REDDIT_MAX_VIDEO_SECONDS", "900"))
 
# Optional: process specific tweet id(s) via environment
PROCESS_TWEET_ID = (os.getenv("PROCESS_TWEET_ID", "") or "").strip()
PROCESS_TWEET_IDS = (os.getenv("PROCESS_TWEET_IDS", "") or "").strip()
PROCESSED_ENV_IDS = set()

# Scheduled weekly player-finder post config (stateless, configurable via env)
def _parse_days_env(val: str) -> set[int]:
    out: set[int] = set()
    try:
        for p in (val or "").split(","):
            p = p.strip()
            if not p:
                continue
            try:
                d = int(p)
                if 1 <= d <= 31:
                    out.add(d)
            except Exception:
                continue
    except Exception:
        pass
    return out or set()

# New: allow scheduling by weekday (0=Mon .. 6=Sun). Default 4 (Friday).
SCHEDULED_PIN_WEEKDAY = int(os.getenv("SCHEDULED_PIN_WEEKDAY", "4"))
# Backward-compat: if SCHEDULED_PIN_DAYS provided, it will be used only if WEEKDAY is invalid (<0 or >6).
SCHEDULED_PIN_DAYS = _parse_days_env(os.getenv("SCHEDULED_PIN_DAYS", ""))
SCHEDULED_PIN_HOUR = int(os.getenv("SCHEDULED_PIN_HOUR", "9"))
SCHEDULED_PIN_TITLE_PREFIX = os.getenv("SCHEDULED_PIN_TITLE_PREFIX", "HaftalÄ±k Oyuncu Arama Ana BaÅŸlÄ±ÄŸÄ± - (")
SCHEDULED_PIN_ENABLED = (os.getenv("SCHEDULED_PIN_ENABLED", "true").strip().lower() == "true")

# PRAW konfigÃ¼rasyonunu Reddit API kurallarÄ±na uygun ÅŸekilde optimize et
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    username=REDDIT_USERNAME,
    password=REDDIT_PASSWORD,
    user_agent=REDDIT_USER_AGENT,
    ratelimit_seconds=60,  # Reddit API: Max 60 requests per minute
    timeout=30,  # Daha kÄ±sa timeout
    check_for_updates=False,
    check_for_async=False
)

# RedditWarp client setup
try:
    # RedditWarp client oluÅŸtur - positional credentials ile username/password authentication
    redditwarp_client = RedditWarpClient(
        REDDIT_CLIENT_ID,
        REDDIT_CLIENT_SECRET,
        REDDIT_USERNAME,
        REDDIT_PASSWORD
    )
    
    # User agent ayarla
    redditwarp_client.http.headers['User-Agent'] = REDDIT_USER_AGENT
    
    print("[+] RedditWarp client baÅŸarÄ±yla kuruldu")
    
except Exception as rw_setup_error:
    print(f"[UYARI] RedditWarp setup hatasÄ±: {rw_setup_error}")
    redditwarp_client = None

def get_random_user_agent():
    """Rastgele User-Agent dÃ¶ndÃ¼r"""
    return random.choice(USER_AGENTS)

def get_proxy():
    """Aktif proxy ayarlarÄ±nÄ± dÃ¶ndÃ¼r"""
    global CURRENT_PROXY_INDEX
    
    if USE_TOR:
        print("[+] Tor proxy kullanÄ±lÄ±yor")
        return TOR_PROXY
    elif USE_PROXY and PROXY_LIST:
        proxy = PROXY_LIST[CURRENT_PROXY_INDEX % len(PROXY_LIST)]
        CURRENT_PROXY_INDEX += 1
        print(f"[+] HTTP proxy kullanÄ±lÄ±yor: {proxy}")
        return proxy
    else:
        return None

def test_proxy(proxy):
    """Proxy'nin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± test et"""
    try:
        response = requests.get('http://httpbin.org/ip', proxies=proxy, timeout=10)
        if response.status_code == 200:
            ip_info = response.json()
            print(f"[+] Proxy Ã§alÄ±ÅŸÄ±yor - IP: {ip_info.get('origin', 'Unknown')}")
            return True
    except Exception as e:
        print(f"[UYARI] Proxy test baÅŸarÄ±sÄ±z: {e}")
    return False

def get_or_create_session(instance_url):
    """ðŸ§¹ Instance iÃ§in session al veya yeni oluÅŸtur (memory optimized)"""
    global SESSION_POOL, SESSION_LAST_USED
    
    current_time = time.time()
    
    # ðŸ§¹ Eski session'larÄ± temizle - memory management
    expired_keys = []
    for key, last_used in SESSION_LAST_USED.items():
        if current_time - last_used > MAX_SESSION_AGE:
            expired_keys.append(key)
    
    for key in expired_keys:
        if key in SESSION_POOL:
            try:
                SESSION_POOL[key].close()
            except Exception:
                pass
            del SESSION_POOL[key]
        del SESSION_LAST_USED[key]
    
    # ðŸ§¹ Temizlik
    del expired_keys
    
    # Mevcut session'Ä± kullan veya yeni oluÅŸtur
    if instance_url not in SESSION_POOL:
        session = requests.Session()
        session.headers.update({
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # Proxy ayarlarÄ±nÄ± ekle
        proxy = get_proxy()
        if proxy:
            if test_proxy(proxy):
                session.proxies.update(proxy)
            # ðŸ§¹ Logging azaltÄ±ldÄ± - sadece hata durumunda log
        
        SESSION_POOL[instance_url] = session
    
    SESSION_LAST_USED[instance_url] = current_time
    return SESSION_POOL[instance_url]

def clean_text(text):
    """Metni temizle ve kÄ±salt"""
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub(r't\.co/\S+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = text.replace('|', '')
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def clean_tweet_text(text):
    if not text:
        return ""
    # RT @TheBFWire: ifadesini kaldÄ±r
    text = re.sub(r'^RT @TheBFWire:\s*', '', text)
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'www\.\S+', '', text)
    text = re.sub(r't\.co/\S+', '', text)
    text = re.sub(r'#\w+', '', text)
    # "via @nickname" ve genel @mention'larÄ± kaldÄ±r
    # via @user (parantez iÃ§inde/sonunda olabilir)
    text = re.sub(r'(?i)\bvia\s+@[-_a-zA-Z0-9]+', '', text)
    # tÃ¼m @mention'larÄ± kaldÄ±r (Ã¶rn: @user, @User_Name)
    text = re.sub(r'@[A-Za-z0-9_]+', '', text)
    # BoÅŸ kalan parantez/dash kalÄ±ntÄ±larÄ±nÄ± toparla
    text = re.sub(r'\(\s*\)', '', text)
    text = re.sub(r'\[\s*\]', '', text)
    text = re.sub(r'\s*[-â€“â€”]\s*$', '', text)
    text = text.replace('|', '')
    # SatÄ±r sonlarÄ±nÄ± koru, sadece aynÄ± satÄ±rdaki fazla boÅŸluklarÄ± temizle
    # \n karakterlerini geÃ§ici olarak koruma altÄ±na al
    text = text.replace('\n', '|||NEWLINE|||')
    text = re.sub(r'\s+', ' ', text)  # Fazla boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
    text = text.replace('|||NEWLINE|||', '\n')  # SatÄ±r sonlarÄ±nÄ± geri getir
    # SatÄ±r baÅŸÄ±/sonundaki gereksiz boÅŸluklarÄ± temizle ama satÄ±r sonlarÄ±nÄ± koru
    lines = text.split('\n')
    lines = [line.strip() for line in lines]
    text = '\n'.join(lines)
    return text.strip()

def extract_username_from_tweet_url(url: str) -> str:
    """Tweet URL'sinden kullanÄ±cÄ± adÄ±nÄ± Ã§Ä±kar.
    Beklenen biÃ§imler:
    - https://x.com/<username>/status/<id>
    - https://twitter.com/<username>/status/<id>
    Uyumlu deÄŸilse varsayÄ±lan olarak TWITTER_SCREENNAME dÃ¶ner.
    """
    try:
        if not url:
            return TWITTER_SCREENNAME
        m = re.search(r"https?://(?:x|twitter)\.com/([^/]+)/status/", url)
        if m:
            return m.group(1)
    except Exception:
        pass
    return TWITTER_SCREENNAME

async def init_twscrape_api():
    """twscrape API'yi baÅŸlat"""
    global twscrape_api
    if twscrape_api is None:
        # Debug: show resolved DB path and basic access
        print(f"[DEBUG] twscrape accounts DB path: {ACCOUNTS_DB_PATH}")
        if not os.path.exists(ACCOUNTS_DB_PATH):
            print("[UYARI] accounts.db bulunamadÄ±, twscrape eriÅŸimi baÅŸarÄ±sÄ±z olabilir")
        elif not os.access(ACCOUNTS_DB_PATH, os.R_OK):
            print("[UYARI] accounts.db dosyasÄ± okunamÄ±yor (izin)")
        # Optional: enable verbose native twscrape logs
        try:
            if os.getenv("TWSCRAPE_DEBUG", "false").strip().lower() == "true":
                # Set up root logger minimally if not configured
                if not logging.getLogger().handlers:
                    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")
                # Verbose for twscrape and httpx
                logging.getLogger("twscrape").setLevel(logging.DEBUG)
                logging.getLogger("twscrape.api").setLevel(logging.DEBUG)
                logging.getLogger("httpx").setLevel(logging.WARNING)
                print("[INFO] TWSCRAPE_DEBUG etkin: twscrape native loglarÄ± DEBUG seviyesinde")
        except Exception:
            pass
        twscrape_api = API(ACCOUNTS_DB_PATH)
        print("[+] twscrape API baÅŸlatÄ±ldÄ±")
    return twscrape_api

async def _get_best_media_urls(tweet_id: int | str) -> tuple[str | None, str | None]:
    """twscrape ile tweet detaylarÄ±ndan en iyi MP4 ve HLS URL'lerini bul.
    DÃ¶nÃ¼ÅŸ: (best_mp4_url, best_hls_url)
    """
    try:
        api = await init_twscrape_api()
        try:
            detail = await asyncio.wait_for(api.tweet_details(int(tweet_id)), timeout=TWSCRAPE_DETAIL_TIMEOUT)
        except Exception as te:
            print(f"[UYARI] tweet_details timeout/hata: {te}")
            detail = None
        if not detail or not getattr(detail, "media", None):
            print("[UYARI] Tweet detayÄ±nda medya bulunamadÄ±")
            return None, None

        videos = getattr(detail.media, "videos", []) or []
        if not videos:
            print("[UYARI] Video medyasÄ± yok")
            return None, None

        mp4_candidates: list[tuple[int, str]] = []
        hls_candidates: list[tuple[int, str]] = []
        for v in videos:
            variants = getattr(v, "variants", []) or []
            if os.getenv("ACCOUNTS_PRINT_VARIANTS", "false").lower() == "true":
                print(f"[DEBUG] Variants: {[getattr(x,'url',None) for x in variants]}")
            for var in variants:
                url = getattr(var, "url", None)
                br = getattr(var, "bitrate", 0) or 0
                if not url:
                    continue
                ul = url.lower()
                if ul.endswith(".m3u8"):
                    hls_candidates.append((br, url))
                elif ".mp4" in ul or ul.endswith(".mp4"):
                    mp4_candidates.append((br, url))

        best_mp4 = max(mp4_candidates, key=lambda x: x[0])[1] if mp4_candidates else None
        best_hls = max(hls_candidates, key=lambda x: x[0])[1] if hls_candidates else None
        if best_mp4:
            print(f"[+] MP4 aday bulundu: {best_mp4}")
        else:
            print("[INFO] MP4 varyantÄ± bulunamadÄ±")
        if best_hls:
            print(f"[+] HLS aday bulundu: {best_hls}")
        return best_mp4, best_hls
    except Exception as e:
        print(f"[HATA] twscrape media Ã§Ã¶zÃ¼mleme hatasÄ±: {e}")
        return None, None

def _download_hls_py(hls_url: str, filename: str) -> str | None:
    """FFmpeg yoksa saf-Python HLS indirme (Render uyumlu)."""
    try:
        playlist = m3u8.load(hls_url)
        if not playlist or not playlist.segments:
            print("[UYARI] HLS playlist boÅŸ veya geÃ§ersiz")
            return None
        base = hls_url.rsplit('/', 1)[0]
        with open(filename, 'wb') as out:
            for seg in playlist.segments:
                surl = seg.uri
                if not surl.startswith('http'):
                    surl = f"{base}/{surl}"
                try:
                    with requests.get(surl, timeout=30, stream=True) as r:
                        if r.status_code == 200:
                            for chunk in r.iter_content(1024 * 64):
                                if chunk:
                                    out.write(chunk)
                        else:
                            print(f"[UYARI] Segment indirilemedi: {surl} -> {r.status_code}")
                            return None
                except Exception as se:
                    print(f"[UYARI] Segment hatasÄ±: {se}")
                    return None
        return filename
    except Exception as e:
        print(f"[HATA] HLS py indirme hatasÄ±: {e}")
        return None

def download_best_video_for_tweet(tweet_id: str | int, out_filename: str) -> str | None:
    """Tweet iÃ§in en kaliteli videoyu indir (HLS tercihli, sonra MP4, opsiyonel yt-dlp)."""
    try:
        # 1) En iyi URL'leri al (async)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            best_mp4, best_hls = loop.run_until_complete(_get_best_media_urls(tweet_id))
        finally:
            loop.close()

        # 2) Ã–nce HLS dene
        if best_hls:
            ffmpeg_path = shutil.which('ffmpeg')
            if ffmpeg_path:
                print("[+] HLS ffmpeg ile indiriliyor...")
                cmd = [ffmpeg_path, '-y', '-i', best_hls, '-c', 'copy', out_filename]
                try:
                    subprocess.run(cmd, check=True, timeout=300)
                    if os.path.exists(out_filename) and os.path.getsize(out_filename) > 0:
                        return out_filename
                except subprocess.CalledProcessError as cpe:
                    print(f"[UYARI] ffmpeg HLS indirimi baÅŸarÄ±sÄ±z: {cpe}")
                except subprocess.TimeoutExpired:
                    print("[UYARI] ffmpeg HLS indirmesi zaman aÅŸÄ±mÄ±")
            # Saf Python HLS
            if os.getenv('USE_PY_HLS', 'true').lower() == 'true':
                print("[+] HLS saf-Python ile indiriliyor...")
                hls_path = _download_hls_py(best_hls, out_filename)
                if hls_path:
                    return hls_path

        # 3) MP4 varyantÄ± ile indir
        if best_mp4:
            print("[+] MP4 varyantÄ± indiriliyor...")
            path = download_media(best_mp4, out_filename)
            if path:
                return path

        # 4) Son Ã§are: yt-dlp (opsiyonel)
        if os.getenv('USE_YTDLP', 'false').lower() == 'true':
            try:
                print("[+] yt-dlp fallback deneniyor...")
                url = f"https://x.com/i/web/status/{tweet_id}"
                cmd = [
                    'yt-dlp', '-f', 'bv*+ba/b[ext=mp4]/bv/best', '-o', out_filename, url
                ]
                subprocess.run(cmd, check=True, timeout=600)
                if os.path.exists(out_filename) and os.path.getsize(out_filename) > 0:
                    return out_filename
            except Exception as yte:
                print(f"[UYARI] yt-dlp fallback baÅŸarÄ±sÄ±z: {yte}")

        print("[UYARI] En kaliteli video indirilemedi")
        return None
    except Exception as e:
        print(f"[HATA] En kaliteli video indirirken: {e}")
        return None

def _is_retweet_of_target(raw_text: str, target_screenname: str) -> bool:
    """Metnin belirli hedef hesabÄ±n retweet'i olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
    - BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf duyarsÄ±z
    - '@hedef' sonrasÄ± ':' opsiyonel
    - Birden fazla alias destekler (virgÃ¼lle ayrÄ±lmÄ±ÅŸ)
    """
    if not raw_text:
        return False
    # Alias listesi: env Ã¼zerinden de verilebilir (Ã¶rn: "bf6_tr,bf6tr,battlefield6tr")
    aliases_env = os.getenv("SECONDARY_RETWEET_TARGET", target_screenname) or target_screenname
    aliases = [a.strip().lstrip('@').lower() for a in aliases_env.split(',') if a.strip()]
    txt = raw_text.strip()
    if not txt.lower().startswith('rt '):
        # RT ifadesi varsa ama farklÄ± biÃ§imde olabilir; yine de hÄ±zlÄ± kontrol
        if 'rt @' not in txt.lower():
            return False
    # Esnek desen: RT [boÅŸluklar] @alias(:| )
    for alias in aliases:
        pattern = rf"^\s*RT\s+@{re.escape(alias)}\b\s*:?(\s|$)"
        if re.search(pattern, txt, flags=re.IGNORECASE):
            return True
    return False

async def _get_bf6_retweets_twscrape(target: str, count: int = 3):
    """bf6_tr (veya SECONDARY_RETWEET_TARGET) kullanÄ±cÄ±sÄ±nÄ±n zaman akÄ±ÅŸÄ±ndan
    retweet olan Ã¶ÄŸeleri bulur ve retweet edilen ORÄ°JÄ°NAL tweet'leri dÃ¶ndÃ¼rÃ¼r.
    """
    try:
        api = await init_twscrape_api()
        # Hedef kullanÄ±cÄ±yÄ± ID ile bulmaya Ã§alÄ±ÅŸ, yoksa login ile dene
        target_id_env = (os.getenv("SECONDARY_RETWEET_TARGET_ID", "") or "").strip()
        user = None
        if target_id_env and target_id_env.isdigit():
            try:
                user = await api.user_by_id(int(target_id_env))
            except Exception as _euid:
                print(f"[UYARI] target user by id alÄ±namadÄ±: {target_id_env} -> {_euid}")
        if not user:
            user = await api.user_by_login(target)
        if not user:
            print(f"[HATA] Hedef kullanÄ±cÄ± bulunamadÄ±: {target} / {target_id_env}")
            return []

        results = []
        detail_lookups = 0
        max_detail_lookups = max(1, count)

        async for tweet in api.user_tweets(user.id, limit=count * 6):
            # Retweet deÄŸilse atla
            rt = getattr(tweet, 'retweetedTweet', None)
            if not rt:
                continue

            # Medya topla (orijinal tweet)
            media_urls = []
            t_media = getattr(rt, 'media', None)
            if not t_media or (
                len(getattr(t_media, 'photos', []) or []) == 0 and
                len(getattr(t_media, 'videos', []) or []) == 0 and
                len(getattr(t_media, 'animated', []) or []) == 0
            ):
                if detail_lookups < max_detail_lookups:
                    try:
                        detail = await api.tweet_details(rt.id, wait=TWSCRAPE_DETAIL_TIMEOUT)
                        detail_lookups += 1
                        if detail and getattr(detail, 'media', None):
                            t_media = detail.media
                    except Exception as de:
                        print(f"[UYARI] Orijinal tweet detay Ã§ekilemedi: {de}")

            if t_media:
                for photo in getattr(t_media, 'photos', []) or []:
                    url = getattr(photo, 'url', None)
                    if url:
                        media_urls.append(url)
                for video in getattr(t_media, 'videos', []) or []:
                    variants = getattr(video, 'variants', []) or []
                    if variants:
                        best = max(variants, key=lambda x: getattr(x, 'bitrate', 0))
                        vurl = getattr(best, 'url', None)
                        if vurl:
                            media_urls.append(vurl)

            tweet_data = {
                'id': str(getattr(rt, 'id', getattr(rt, 'id_str', ''))),
                'text': getattr(rt, 'rawContent', ''),
                'created_at': getattr(rt, 'date', None),
                'media_urls': media_urls,
                'url': getattr(rt, 'url', None)
            }

            results.append(tweet_data)
            if len(results) >= count:
                break

        # Eskiden yeniye sÄ±rala
        def _tw_key(td):
            ts = td.get('created_at') if isinstance(td, dict) else None
            tsv = 0
            try:
                if hasattr(ts, 'timestamp'):
                    tsv = int(ts.timestamp())
            except Exception:
                tsv = 0
            try:
                return (tsv, int(str(td.get('id', '0'))))
            except Exception:
                return (tsv, 0)

        results.sort(key=_tw_key)
        return results
    except Exception as e:
        print(f"[UYARI] @bf6_tr retweet'leri alÄ±namadÄ± (async): {e}")
        return []

def get_latest_bf6_retweets(count: int = 3):
    """twscrape ile TWITTER_SCREENNAME zaman akÄ±ÅŸÄ±ndan sadece @bf6_tr retweet'lerini getirir.
    BaÅŸarÄ±sÄ±z olursa sessizce boÅŸ liste dÃ¶ner. Mevcut pipeline ile aynÄ± veri ÅŸeklini Ã¼retir.
    """
    target = os.getenv("SECONDARY_RETWEET_TARGET", "bf6_tr")
    try:
        # Rate limiting (aynÄ± mekanizma)
        global LAST_REQUEST_TIME
        current_time = time.time()
        time_since_last_request = current_time - LAST_REQUEST_TIME
        if time_since_last_request < MIN_REQUEST_INTERVAL:
            wait_time = MIN_REQUEST_INTERVAL - time_since_last_request
            print(f"[+] (RT) Rate limiting: {int(wait_time)} saniye bekleniyor...")
            time.sleep(wait_time)
        LAST_REQUEST_TIME = time.time()

        # Async twscrape Ã§aÄŸrÄ±sÄ±
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            tweets = loop.run_until_complete(_get_bf6_retweets_twscrape(target, count))
        finally:
            loop.close()
        return tweets or []
    except Exception as e:
        print(f"[UYARI] @bf6_tr retweet'leri alÄ±namadÄ±: {e}")
        return []


# Pnytter fallback fonksiyonu kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak

# RSS fallback fonksiyonu kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak

def get_media_urls_from_tweet_data(tweet_data):
    """ðŸ§¹ TWSCRAPE'den alÄ±nan tweet verisinden medya URL'lerini Ã§Ä±kar"""
    if not tweet_data or "media_urls" not in tweet_data:
        return []
    
    try:
        media_urls = tweet_data.get("media_urls", [])
        return media_urls
        
    except Exception:
        return []

# ðŸ§¹ Nitter HTML fonksiyonlarÄ± kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak

# ðŸ§¹ Nitter instance yÃ¶netim fonksiyonlarÄ± kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak

def process_specific_tweet(tweet_id: str) -> dict:
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            async def _run():
                api = await init_twscrape_api()
                try:
                    detail = await asyncio.wait_for(api.tweet_details(int(tweet_id)), timeout=TWSCRAPE_DETAIL_TIMEOUT)
                except Exception:
                    detail = None
                if not detail:
                    return {"processed": False, "reason": "detail_not_found"}
                if getattr(detail, 'inReplyToTweetId', None):
                    return {"processed": False, "reason": "is_reply"}
                if getattr(detail, 'retweetedTweet', None):
                    return {"processed": False, "reason": "is_retweet"}
                if getattr(detail, 'quotedTweet', None) or getattr(detail, 'isQuoted', False) or getattr(detail, 'isQuote', False):
                    return {"processed": False, "reason": "is_quote"}

                media_urls = []
                md = getattr(detail, 'media', None)
                if md:
                    photos = getattr(md, 'photos', []) or []
                    for p in photos:
                        u = getattr(p, 'url', None)
                        if u:
                            media_urls.append(u)
                    videos = getattr(md, 'videos', []) or []
                    for v in videos:
                        vars = getattr(v, 'variants', []) or []
                        if vars:
                            best = max(vars, key=lambda x: getattr(x, 'bitrate', 0))
                            u = getattr(best, 'url', None)
                            if u:
                                media_urls.append(u)
                    animated = getattr(md, 'animated', []) or []
                    for a in animated:
                        u = getattr(a, 'videoUrl', None)
                        if u:
                            media_urls.append(u)

                tweet_data = {
                    'id': str(getattr(detail, 'id', getattr(detail, 'id_str', ''))),
                    'text': getattr(detail, 'rawContent', ''),
                    'created_at': getattr(detail, 'date', None),
                    'media_urls': media_urls,
                    'url': getattr(detail, 'url', None),
                }

                text = tweet_data.get("text", "")
                cleaned_text = clean_tweet_text(text)
                try:
                    cd_days = _extract_countdown_days(text) or _extract_countdown_days(cleaned_text)
                except Exception:
                    cd_days = None
                if cd_days is not None and cd_days > 10:
                    return {"processed": False, "reason": "countdown_gt_10"}

                media_urls2 = get_media_urls_from_tweet_data(tweet_data)
                media_files = []
                image_urls = []
                video_urls = []
                for mu in media_urls2:
                    ul = str(mu).lower()
                    is_image = (
                        any(ext in ul for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']) or
                        'format=jpg' in ul or 'format=jpeg' in ul or 'format=png' in ul or 'format=webp' in ul or
                        'pbs.twimg.com/media' in ul
                    )
                    is_video = ('.mp4' in ul or 'format=mp4' in ul or 'video.twimg.com' in ul)
                    if is_image:
                        image_urls.append(mu)
                    elif is_video:
                        video_urls.append(mu)

                translated = translate_text(cleaned_text, has_video=bool(video_urls))
                if not translated:
                    return {"processed": False, "reason": "translation_failed"}

                if len(image_urls) > 1:
                    downloaded_images = download_multiple_images(image_urls, tweet_id)
                    media_files.extend(downloaded_images)
                elif len(image_urls) == 1:
                    media_url = image_urls[0]
                    ext = os.path.splitext(media_url)[1].split("?")[0] or ".jpg"
                    filename = f"temp_image_{tweet_id}_0{ext}"
                    path = download_media(media_url, filename)
                    if path:
                        media_files.append(path)

                if video_urls:
                    filename = f"temp_video_{tweet_id}_0.mp4"
                    path = download_best_video_for_tweet(tweet_id, filename)
                    if path:
                        dur = get_video_duration_seconds(path)
                        if dur is not None and dur > REDDIT_MAX_VIDEO_SECONDS:
                            try:
                                if os.path.exists(path):
                                    os.remove(path)
                            except Exception:
                                pass
                            for fpath in media_files:
                                try:
                                    if os.path.exists(fpath):
                                        os.remove(fpath)
                                except Exception:
                                    pass
                            return {"processed": False, "reason": "video_too_long"}
                        converted = f"converted_{filename}"
                        converted_path = convert_video_to_reddit_format(path, converted)
                        if converted_path:
                            media_files.append(converted_path)
                        if os.path.exists(path):
                            os.remove(path)

                original_text = tweet_data.get("text", "")
                has_media_in_original = any(ind in original_text for ind in ['pic.twitter.com', 'video.twitter.com', 'pbs.twimg.com'])
                if has_media_in_original and len(media_files) == 0:
                    for fpath in media_files:
                        try:
                            if os.path.exists(fpath):
                                os.remove(fpath)
                        except Exception:
                            pass
                    return {"processed": False, "reason": "media_expected_but_missing"}

                candidates = [
                    (translated or "").strip(),
                    (cleaned_text or "").strip(),
                    (text or "").strip(),
                ]
                chosen_text = next((c for c in candidates if c), "")
                if not chosen_text:
                    chosen_text = f"@{TWITTER_SCREENNAME} paylaÅŸÄ±mÄ± - {tweet_id}"
                title_to_use, remainder_to_post = smart_split_title(chosen_text, 300)
                ok = submit_post(title_to_use, media_files, text, remainder_text=remainder_to_post)
                if ok:
                    return {"processed": True, "tweet_id": str(tweet_id), "title": title_to_use}
                return {"processed": False, "reason": "submit_failed"}

            result = loop.run_until_complete(_run())
            return result
        finally:
            loop.close()
    except Exception as e:
        return {"processed": False, "reason": f"exception:{e}"}


# -------------------- Web Service (FastAPI) --------------------
# ðŸ§¹ Lazy import - FastAPI sadece ihtiyaÃ§ duyulduÄŸunda import edilecek

# ðŸ§¹ Global deÄŸiÅŸkenler
_worker_started = False
_worker_lock = None
app = None

def get_instance_health_status() -> bool:
    """Basit saÄŸlÄ±k durumu: arka plan iÅŸÃ§isi baÅŸladÄ± mÄ±?"""
    try:
        return bool(_worker_started)
    except Exception:
        # VarsayÄ±lan: servis ayakta kabul et
        return True

def _init_fastapi():
    """ðŸ§¹ FastAPI lazy initialization"""
    global app, _worker_lock
    if app is None:
        # ðŸ§¹ Lazy import
        try:
            from fastapi import FastAPI, Request
            from fastapi.responses import PlainTextResponse
            import threading
        except ImportError:
            raise RuntimeError("FastAPI veya threading mevcut deÄŸil")
        
        app = FastAPI(title="X-to-Reddit Bot")
        _worker_lock = threading.Lock()
        
        # Route'larÄ± tanÄ±mla
        @app.api_route("/", methods=["GET", "HEAD"], response_class=PlainTextResponse)
        def root(request: Request):
            if request.method == "HEAD":
                return PlainTextResponse("", status_code=200)
            return "OK"

        @app.api_route("/healthz", methods=["GET", "HEAD"], response_class=PlainTextResponse)
        def healthz(request: Request):
            try:
                if request.method == "HEAD":
                    return PlainTextResponse("", status_code=200)
                status = get_instance_health_status()
                return "healthy" if status else "ok"
            except Exception:
                return PlainTextResponse("ok", status_code=200)

        @app.api_route("/ping", methods=["GET", "HEAD"])
        async def ping(request: Request):
            # Basit canlÄ±lÄ±k kontrolÃ¼
            if request.method == "HEAD":
                return PlainTextResponse("", status_code=200)
            return {"status": "alive"}

        @app.api_route("/process/{tweet_id}", methods=["GET", "POST"])
        async def process_tweet_endpoint(tweet_id: str):
            try:
                # Senkron yardÄ±mcÄ± fonksiyonu Ã§aÄŸÄ±r
                res = process_specific_tweet(tweet_id)
                return res
            except Exception as e:
                return {"processed": False, "reason": f"exception:{e}"}

        @app.on_event("startup")
        def start_background_worker():
            global _worker_started
            with _worker_lock:
                if _worker_started:
                    return
                # DB preflight: if DB is mandatory, ensure we can connect and create table
                if USE_DB_FOR_POSTED_IDS and FAIL_IF_DB_UNAVAILABLE:
                    try:
                        _ensure_posted_ids_table()
                    except Exception as e:
                        # Fail fast: exit the process so Render restarts with proper env
                        try:
                            sys.exit(1)
                        except SystemExit:
                            raise
                        except Exception:
                            # As a fallback, re-raise to stop background worker
                            raise
                def _run():
                    # main_loop zaten kendi iÃ§inde sonsuz dÃ¶ngÃ¼ye sahip
                    try:
                        main_loop()
                    except Exception:
                        pass  # ðŸ§¹ Logging azaltÄ±ldÄ±
                t = threading.Thread(target=_run, name="bot-worker", daemon=True)
                t.start()
                _worker_started = True
    
    return app

# Expose ASGI app for 'uvicorn bot:app' imports (Render)
# This keeps local execution via `python bot.py` working as well.
app = _init_fastapi()

# ðŸ§¹ Route tanÄ±mlarÄ± _init_fastapi() fonksiyonuna taÅŸÄ±ndÄ±

# ðŸ§¹ Nitter multi-instance fonksiyonu kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak

# ðŸ§¹ Gallery-dl fonksiyonu kaldÄ±rÄ±ldÄ± - sadece TWSCRAPE kullanÄ±lacak

def translate_text(text, has_video: bool = False):
    """ðŸ§¹ Gemini 2.5 Flash ile Ä°ngilizce -> TÃ¼rkÃ§e Ã§eviri (memory optimized)
    Ã‡Ä±kÄ±ÅŸ: Sadece ham Ã§eviri (ek aÃ§Ä±klama, tÄ±rnak, etiket vs. yok).
    Ã–zel terimleri Ã‡EVÄ°RME: battlefield, free pass, battle pass.
    has_video: Kaynak tweet'te video varsa True (Ã¶r: 'reload' -> 'ÅžarjÃ¶r').
    """
    try:
        if not text or not text.strip():
            return None
        
        # ðŸ§¹ Lazy import - sadece ihtiyaÃ§ duyulduÄŸunda import et
        try:
            from google import genai
            from google.genai import types
        except ImportError:
            return None
        
        # Normalize input to NFC first
        original_text = _nfc(text)
        # If text is shouty ALL CAPS, create a de-shouted version for translation
        input_for_translation = _deshout_en_sentence_case(original_text) if _is_all_caps_like(original_text) else original_text

        # Cache lookup (normalized key)
        key = input_for_translation.strip()
        with TRANSLATE_CACHE_LOCK:
            cached = TRANSLATE_CACHE.get(key)
            if cached is not None:
                # touch LRU order if available
                try:
                    TRANSLATE_CACHE.move_to_end(key)
                except Exception:
                    pass
                return cached

        # ðŸ§¹ Gemini client - lazy initialization
        client = None
        try:
            client = genai.Client()
        except Exception:
            return None
        # Modeller: primary ve fallback env ile ayarlanabilir
        model_primary = os.getenv("GEMINI_MODEL_PRIMARY", "gemini-2.5-flash-lite").strip()
        model_fallback = os.getenv("GEMINI_MODEL_FALLBACK", "gemini-2.5-flash").strip()

        # Talimat: sadece ham Ã§eviri, belirli terimler Ã§evrilmez.
        # BaÄŸlam satÄ±rÄ±: video var/yok bilgisi ile Ã¶zel kurallar uygulanÄ±r.
        prompt = (
            "Translate the text from English (source: en) to Turkish (target: tr). Output ONLY the translation with no extra words, "
            "no quotes, no labels. Do NOT translate these terms and keep their original casing: "
            "Battlefield, Free Pass, Battle Pass, Operation Firestorm, Easter Egg, Plus, Trickshot, Support, Recon, Assault, Engineer.\n"
            "Preserve the original tweet's capitalization EXACTLY for all words where possible; do not change upper/lower casing from the source text, "
            "but apply strict capitalization preservation ONLY to protected terms and proper nouns; Turkish words should use natural Turkish casing.\n"
            "Translate ALL parts of the text into Turkish EXCEPT the protected terms listed above. Do NOT leave any sentence or common word in English.\n"
            "If the input includes any mentions like @nickname or patterns like 'via @nickname', exclude them from the output entirely.\n"
            "If the content appears to be a short gameplay/clip highlight rather than a news/article, compress it into ONE coherent Turkish sentence (no bullet points, no multiple sentences).\n"
            "Special rule: If the text is exactly 'W', translate it as 'Ä°yi'; if exactly 'L', translate it as 'KÃ¶tÃ¼'; if the text is 'W or L?' or 'W/L?', translate it as 'Ä°yi mi, KÃ¶tÃ¼ mÃ¼?'.\n"
            "Fidelity rules: Do NOT invent or change weapon types or roles. Translate phrases like 'snipes' as a neutral 'vuruyor/Ã¶ldÃ¼rÃ¼yor' unless a SNIPER RIFLE is explicitly mentioned.\n"
            "Translate 'using a [WEAPON]' exactly as 'bir [WEAPON] kullanarak' and keep the weapon type accurate (e.g., 'using a missile launcher' => 'bir roketatar/fÃ¼zeatar kullanarak').\n"
            "Maintain the subject/object roles from the source (e.g., '[Game] player snipes a parachuting player' => '[Game] oyuncusu, paraÅŸÃ¼tle inen bir oyuncuyu ...'). Do NOT swap actor and target.\n"
            "Absolutely do NOT add emojis, usernames, sources, credits, or extra markers (e.g., 'ðŸŽ¥', 'u/username', 'via ...'). Output only the translated sentence.\n"
            "Special phrases like 'Day 1' should be translated contextually: use 'Ã‡Ä±kÄ±ÅŸ gÃ¼nÃ¼' or other natural Turkish phrase when referring to game launch. "
            "Other idiomatic phrases like 'now that X is purchasable' should be rendered smoothly in Turkish, e.g., 'artÄ±k X mevcut olduÄŸundan' or 'X satÄ±n alÄ±nabildiÄŸi iÃ§in'.\n"
            "Additionally, if the source text contains these tags/keywords, translate them EXACTLY as follows (preserve casing where appropriate):\n"
            "BREAKING => SON DAKÄ°KA; LEAK (as a standalone tag/label) => SIZINTI; HUMOUR => SÃ–YLENTI; NEW => YENÄ°.\n"
            "When these tags appear at the beginning of text (e.g., 'BREAKING:', 'NEW:', 'LEAK:'), keep them in ALL UPPERCASE in Turkish (e.g., 'SON DAKÄ°KA:', 'YENÄ°:', 'SIZINTI:').\n"
            "CRITICAL: For the verb forms 'leak/leaked/has leaked/has been leaked', render them naturally in Turkish as a verb: prefer 'sÄ±zdÄ±'.\n"
            "Use 'sÄ±zdÄ±rÄ±ldÄ±' ONLY if the English clearly states an explicit agent causing the leak (e.g., 'was leaked by X').\n"
            "NEVER output the awkward phrase 'sÄ±zÄ±ntÄ± oldu'.\n"
            "For gaming phrasing, translate 'Intro Gameplay' as 'giriÅŸ oynanÄ±ÅŸÄ±' (or 'aÃ§Ä±lÄ±ÅŸ oynanÄ±ÅŸÄ±' if it reads more naturally in context).\n"
            "Remove any first-person opinions or subjective phrases (e.g., 'I think', 'IMO', 'bence', 'bana gÃ¶re'); keep only neutral, factual content.\n"
            "Before finalizing, re-read your Turkish output and ensure it is coherent and faithful: do NOT invent numbers, durations (e.g., '3-5 gÃ¼n'), hedging words (e.g., 'sanki', 'gibi', 'muhtemelen') unless they EXIST in the English. Remove any such additions. Do NOT add or change meaning.\n"
            "Do not translate 'Campaign' in a video game context as 'Kampanya'; prefer 'Hikaye' (or 'Hikaye modu' if fits better). Translate 'Campaign Early Access' as 'Hikaye Erken EriÅŸimi'.\n"
            f"Context: HAS_VIDEO={'true' if has_video else 'false'} â€” If HAS_VIDEO is true AND the English contains the word 'reload', translate 'reload' specifically as 'ÅžarjÃ¶r' (capitalize S). Otherwise, translate naturally (do NOT use 'ÅžarjÃ¶r').\n"
            "Before finalizing, ensure the Turkish output is coherent and natural; do NOT produce two unrelated sentences or add stray quoted fragments. If any part seems odd, fix it for clarity while staying faithful to the source.\n\n"
            "Important: When translating phrases like 'your [THING] rating', do NOT add Turkish possessive suffixes to game/brand names. Prefer the structure '[NAME] iÃ§in ... derecelendirmeniz' instead of '[NAME]'nÄ±zÄ±n ...'.\n"
            "Example: 'What is your FINAL Rating of the Battlefield 6 Beta? (1-10)' -> 'Battlefield 6 Beta iÃ§in FINAL derecelendirmeniz nedir? (1-10)'.\n\n"
            "Idioms: Translate 'can't wait' / 'cannot wait' / 'can NOT wait' as positive excitement -> 'sabÄ±rsÄ±zlanÄ±yorum' (NOT 'sabÄ±rsÄ±zlanamam'). If the English uses emphasis (e.g., NOT in caps), you may emphasize the Turkish verb (e.g., SABIRSIZLANIYORUM) but do not change the meaning to negative.\n"
            "Meme pattern '... be like': Translate patterns such as 'waiting BF6 be like...' as 'BF6â€™yÄ± beklemek bÃ¶yle bir ÅŸey...' or '[X] bÃ¶yle bir ÅŸey...' Do NOT produce literal 'bekliyorum sanki' or similar unnatural phrasing.\n"
            "Consistency: Never introduce or switch to a different game/series/version that is not in the source. If the source mentions 'Battlefield 2042', do not output 'Battlefield 6', and vice versa. Keep titles and versions consistent with the input.\n"
            "Natural wording: Translate generic English gaming terms to proper Turkish instead of mixing languages (e.g., translate 'cosmetics' as 'kozmetikler' when not a protected proper noun; avoid forms like 'Cosmetics'ler'). Keep protected terms listed above in English as instructed.\n"
            "Use correct Turkish diacritics (Ã§, ÄŸ, Ä±, Ä°, Ã¶, ÅŸ, Ã¼) and keep Unicode in NFC form. Preserve basic punctuation and line breaks.\n\n"
            "Text:\n" + input_for_translation.strip()
        )

        def _translate_with(model_name: str):
            resp = client.models.generate_content(
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(thinking_budget=0)
                ),
            )
            out_local = _nfc((resp.text or "").strip())
            if out_local and out_local != text.strip():
                # Cache write-through + basit LRU tahliyesi
                try:
                    with TRANSLATE_CACHE_LOCK:
                        TRANSLATE_CACHE[key] = out_local
                        try:
                            TRANSLATE_CACHE.move_to_end(key)
                        except Exception:
                            pass
                        try:
                            while len(TRANSLATE_CACHE) > TRANSLATE_CACHE_MAX:
                                TRANSLATE_CACHE.popitem(last=False)
                        except Exception:
                            pass
                except Exception:
                    pass
                return out_local
            return None

        # Ã–nce primary modeli dene
        out = None
        try:
            out = _translate_with(model_primary)
        except Exception as e:
            print(f"[UYARI] Gemini model hata ({model_primary}): {e}")

        # BaÅŸarÄ±sÄ±zsa fallback modeli dene
        if not out and model_fallback and model_fallback != model_primary:
            try:
                out = _translate_with(model_fallback)
            except Exception as e:
                print(f"[UYARI] Gemini model hata ({model_fallback}): {e}")

        if out:
            # If original was ALL CAPS, avoid re-uppercasing Turkish output blindly; keep natural casing
            return out
        print("[UYARI] Ã‡eviri boÅŸ dÃ¶ndÃ¼ veya orijinal ile aynÄ±")
        return None
    except Exception as e:
        print(f"[UYARI] Gemini Ã§eviri hatasÄ±: {e}")
        return None

# --- Joke note detection & auto-comment ---
def _extract_joke_note(text: str) -> str | None:
    """Heuristically detect a short 'joke' note from the original tweet text.
    Returns a concise line to translate and post, or None.
    """
    try:
        if not text:
            return None
        t = (text or "").strip()
        low = t.lower()
        # Markers in TR/EN commonly used by authors
        markers = [
            "ÅŸaka", "saka", "espri", "mizah", "ÅŸakaydÄ±", "ÅŸaka yapÄ±yorum",
            "joke", "jk", "kidding", "just kidding", "this is a joke", "parody", "satire"
        ]
        if not any(m in low for m in markers):
            return None
        # Try to extract the sentence containing the marker
        # Simple split by punctuation and line breaks
        import re as _re
        parts = _re.split(r"(?<=[.!?\n])\s+", t)
        for p in parts:
            pl = p.lower()
            if any(m in pl for m in markers):
                # Keep a concise snippet
                return p.strip()[:300]
        # Fallback to entire text if markers exist but sentence split missed
        return t[:300]
    except Exception:
        return None

def _maybe_post_joke_comment(praw_submission_or_id, title: str, subreddit_name: str, original_tweet_text: str):
    """If the original tweet contains a joke note, translate and post as a comment.
    Accepts either a PRAW Submission, an ID string, or attempts title-based lookup.
    """
    try:
        note = _extract_joke_note(original_tweet_text)
        if not note:
            return
        # Translate to Turkish if it's not already
        translated = translate_text(note) or note
        comment_text = f"Not: Tweet sahibi ÅŸaka yaptÄ±ÄŸÄ±nÄ± belirtiyor â€” {translated}"
        # Resolve submission
        subm = None
        if hasattr(praw_submission_or_id, "reply"):
            subm = praw_submission_or_id
        elif isinstance(praw_submission_or_id, str) and praw_submission_or_id:
            try:
                subm = reddit.submission(id=praw_submission_or_id)
            except Exception:
                subm = None
        # As a last resort, try find by recent posts with same title
        if not subm and title:
            try:
                sr_obj = reddit.subreddit(subreddit_name)
                for s in sr_obj.new(limit=10):
                    author_name = getattr(s.author, 'name', '') or ''
                    if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                        subm = s
                        break
            except Exception:
                pass
        if subm:
            try:
                subm.reply(comment_text)
                print("[+] Åžaka notu yorum olarak eklendi")
            except Exception as ce:
                print(f"[UYARI] Åžaka notu yorum eklenemedi: {ce}")
    except Exception as e:
        print(f"[UYARI] Åžaka notu iÅŸlenemedi: {e}")

# AI-powered flair selection system
FLAIR_OPTIONS = {
    "Haberler": "a3c0f742-22de-11f0-9e24-7a8b08eb260a",
    "Klip": "b6d04ac2-22de-11f0-9536-c6a33f70974b",
    "TartÄ±ÅŸma": "c22e9cfc-22de-11f0-950d-4ee5c0d1016f",
    "Soru": "ccbc6fb4-22de-11f0-b443-da5b1d3016fa",
    "Ä°nceleme": "e52aa2a0-22de-11f0-abed-aa5bfc354624",
    "Kampanya": "26a6deda-68ab-11f0-8584-6a05febc585d",
    "Arkaplan": "33ea1cfa-69c4-11f0-8376-9a5b50ce03e6",
    "SÄ±zÄ±ntÄ±": "351fe58c-6be0-11f0-bcb4-9e6d710db689"
}

# Reverse lookup for flair name by hardcoded ID (to recover name if IDs drift)
_FLAIR_NAME_BY_ID = {vid: k for k, vid in FLAIR_OPTIONS.items()}

# Cache subreddit flair templates to avoid rate limit
_FLAIR_TEMPLATE_CACHE: dict[str, list[dict]] = {}

def _fetch_link_flair_templates(subreddit_name: str) -> list[dict]:
    """Fetch available link flair templates for a subreddit using PRAW.
    Returns a list of dicts with keys like 'id' and 'text'. Caches results per process.
    """
    try:
        if subreddit_name in _FLAIR_TEMPLATE_CACHE and _FLAIR_TEMPLATE_CACHE[subreddit_name]:
            return _FLAIR_TEMPLATE_CACHE[subreddit_name]
        sr = reddit.subreddit(subreddit_name)
        # PRAW: SubredditFlair link templates
        # Some versions expose via sr.flair.link_templates
        templates = []
        try:
            templates = list(getattr(sr.flair, 'link_templates', []) or [])
        except Exception:
            pass
        # Fallback: sr.flair.templates may include both; filter link flairs if present
        if not templates:
            try:
                templates = list(getattr(sr.flair, 'templates', []) or [])
            except Exception:
                templates = []
        # Normalize
        norm = []
        for t in templates:
            # PRAW returns objects or dicts depending on version
            tid = getattr(t, 'id', None) if not isinstance(t, dict) else t.get('id')
            text = getattr(t, 'text', None) if not isinstance(t, dict) else t.get('text')
            if tid and text is not None:
                norm.append({'id': str(tid), 'text': str(text)})
        _FLAIR_TEMPLATE_CACHE[subreddit_name] = norm
        print(f"[INFO] r/{subreddit_name} mevcut link flair'leri: {[n['text']+':'+n['id'] for n in norm]}")
        return norm
    except Exception as e:
        print(f"[UYARI] Flair template'larÄ± alÄ±namadÄ± (r/{subreddit_name}): {e}")
        return []

def _resolve_flair_id_for_subreddit(subreddit_name: str, flair_id: str | None, flair_name_guess: str | None) -> str | None:
    """Ensure flair_id exists on subreddit; if not, try to find by flair text.
    Returns a valid flair_id or None if not resolvable.
    """
    try:
        templates = _fetch_link_flair_templates(subreddit_name)
        if not templates:
            return None
        ids = {t['id'] for t in templates}
        if flair_id and str(flair_id) in ids:
            return str(flair_id)
        # Try by name guess (case-insensitive)
        if flair_name_guess:
            fl = flair_name_guess.strip().lower()
            # Prefer exact text match
            for t in templates:
                if t['text'].strip().lower() == fl:
                    return t['id']
            # Then substring match
            for t in templates:
                tt = t['text'].strip().lower()
                if fl in tt or tt in fl:
                    return t['id']
        # If we had an ID not present, try inferring name from our reverse map
        if flair_id and flair_id in _FLAIR_NAME_BY_ID:
            inferred = _FLAIR_NAME_BY_ID[flair_id].strip().lower()
            for t in templates:
                if t['text'].strip().lower() == inferred:
                    return t['id']
        return None
    except Exception as e:
        print(f"[UYARI] Flair ID Ã§Ã¶zÃ¼mlenemedi: {e}")
        return None

def select_flair_with_ai(title, original_tweet_text="", has_video: bool = False):
    """AI ile otomatik flair seÃ§imi"""
    print("[+] AI ile flair seÃ§imi baÅŸlatÄ±lÄ±yor...")
    print(f"[DEBUG] BaÅŸlÄ±k: {title}")
    print(f"[DEBUG] Orijinal tweet: {original_tweet_text[:100]}..." if original_tweet_text else "[DEBUG] Orijinal tweet yok")
    print(f"[DEBUG] Video var mÄ±: {'Evet' if has_video else 'HayÄ±r'}")
    
    # Ã–nce basit kural tabanlÄ± flair seÃ§imi deneyelim
    title_lower = title.lower()
    tweet_lower = original_tweet_text.lower() if original_tweet_text else ""
    combined_text = f"{title_lower} {tweet_lower}"
    
    print(f"[DEBUG] Analiz edilen metin: {combined_text[:200]}...")
    
    # Kural tabanlÄ± flair seÃ§imi
    if any(word in combined_text for word in ["klip", "gameplay"]):
        selected_flair = "Klip"
    elif any(word in combined_text for word in ["leak", "sÄ±zÄ±ntÄ±", "rumor", "sÃ¶ylenti"]):
        selected_flair = "SÄ±zÄ±ntÄ±"
    elif any(word in combined_text for word in ["campaign", "kampanya", "single player"]):
        selected_flair = "Kampanya"
    elif any(word in combined_text for word in ["review", "inceleme", "deÄŸerlendirme"]):
        selected_flair = "Ä°nceleme"
    elif any(word in combined_text for word in ["question", "soru", "help", "yardÄ±m"]):
        selected_flair = "Soru"
    elif any(word in combined_text for word in ["discussion", "tartÄ±ÅŸma", "opinion", "gÃ¶rÃ¼ÅŸ"]):
        selected_flair = "TartÄ±ÅŸma"
    elif any(word in combined_text for word in ["arkaplan", "background"]):
        selected_flair = "Arkaplan"
    else:
        selected_flair = "Haberler"  # VarsayÄ±lan
    
    selected_flair_id = FLAIR_OPTIONS[selected_flair]
    print(f"[+] Kural tabanlÄ± flair seÃ§imi: {selected_flair} (ID: {selected_flair_id})")
    
    # OpenAI API'yi dene (opsiyonel)
    try:
        # API key kontrolÃ¼
        ai_api_key = os.getenv("OPENAI_API_KEY")
        if not ai_api_key:
            print("[!] OPENAI_API_KEY bulunamadÄ±, Gemini ile deneniyor")
            try:
                # Gemini istemcisi ve model
                gclient = genai.Client()
                g_model_primary = os.getenv("GEMINI_MODEL_PRIMARY", "gemini-2.5-flash-lite").strip()
                g_model_fallback = os.getenv("GEMINI_MODEL_FALLBACK", "gemini-2.5-flash").strip()

                g_prompt = (
                    "AÅŸaÄŸÄ±daki iÃ§eriÄŸi analiz et ve en uygun Reddit flair'ini seÃ§. Sadece aÅŸaÄŸÄ±daki seÃ§eneklerden BÄ°RÄ°NÄ° aynen dÃ¶ndÃ¼r (baÅŸka hiÃ§bir ÅŸey yazma):\n"
                    "Haberler | Klip | TartÄ±ÅŸma | Soru | Ä°nceleme | Kampanya | Arkaplan | SÄ±zÄ±ntÄ±\n\n"
                    "Kural: EÄŸer video VAR ve metin haber/duyuru gibi deÄŸilse 'Klip' seÃ§eneÄŸine Ã¶ncelik ver. Haber duyurusu ise 'Haberler' uygundur.\n\n"
                    f"BaÅŸlÄ±k: {title}\n"
                    f"Video: {'Evet' if has_video else 'HayÄ±r'}\n"
                    + (f"Orijinal Tweet: {original_tweet_text}\n" if original_tweet_text else "") +
                    "YalnÄ±zca seÃ§imi dÃ¶ndÃ¼r."
                )

                def _ask_gemini(model_name: str) -> str:
                    resp = gclient.models.generate_content(
                        model=model_name,
                        contents=g_prompt,
                        config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(thinking_budget=0)
                        ),
                    )
                    return (getattr(resp, 'text', '') or '').strip()

                ai_suggestion = ""
                try:
                    ai_suggestion = _ask_gemini(g_model_primary)
                except Exception as ge1:
                    print(f"[UYARI] Gemini flair (primary) hata: {ge1}")
                if not ai_suggestion and g_model_fallback and g_model_fallback != g_model_primary:
                    try:
                        ai_suggestion = _ask_gemini(g_model_fallback)
                    except Exception as ge2:
                        print(f"[UYARI] Gemini flair (fallback) hata: {ge2}")

                if ai_suggestion:
                    ai_clean = ai_suggestion.strip().strip('#').strip('"').strip()
                    # Tam eÅŸleÅŸme veya iÃ§erme ile eÅŸleÅŸtir
                    for flair_name, flair_id in FLAIR_OPTIONS.items():
                        if ai_clean.lower() == flair_name.lower() or flair_name.lower() in ai_clean.lower() or ai_clean.lower() in flair_name.lower():
                            print(f"[+] Gemini seÃ§ilen flair: {flair_name} (ID: {flair_id})")
                            return flair_id
                    print(f"[!] Gemini Ã¶nerisi eÅŸleÅŸmedi ({ai_clean}), kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor: {selected_flair}")
                    return selected_flair_id
                else:
                    print("[!] Gemini sonuÃ§ Ã¼retmedi, kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor")
                    return selected_flair_id
            except Exception as ge:
                print(f"[UYARI] Gemini fallback genel hata: {ge}")
                return selected_flair_id
        
        # OpenAI API iÃ§in prompt hazÄ±rla
        content_to_analyze = f"BaÅŸlÄ±k: {title}\nVideo: {'Evet' if has_video else 'HayÄ±r'}"
        if original_tweet_text:
            content_to_analyze += f"\nOrijinal Tweet: {original_tweet_text}"
        
        prompt = f"""AÅŸaÄŸÄ±daki Battlefield 6 ile ilgili iÃ§eriÄŸi analiz et ve en uygun flair'i seÃ§.

Kurallar:
- EÄŸer video VAR ve metin haber/duyuru gibi deÄŸilse 'Klip' seÃ§eneÄŸine Ã¶ncelik ver.
- Haber/duyuru ise 'Haberler' uygundur.

Ä°Ã§erik:
{content_to_analyze}

Sadece ÅŸu seÃ§eneklerden birini dÃ¶ndÃ¼r: Haberler, Klip, TartÄ±ÅŸma, Soru, Ä°nceleme, Kampanya, Arkaplan, SÄ±zÄ±ntÄ±
Sadece flair adÄ±nÄ± yaz (Ã¶rnek: Haberler). BaÅŸka bir ÅŸey yazma."""
        
        # OpenAI API Ã§aÄŸrÄ±sÄ±
        url = "https://api.openai.com/v1/chat/completions"
        payload = {
            "model": "gpt-4o-mini",  # Daha ekonomik model
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 50,
            "temperature": 0.1
        }
        headers = {
            "Authorization": f"Bearer {ai_api_key}",
            "Content-Type": "application/json"
        }
        
        print("[+] OpenAI API Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor...")
        response = requests.post(url, json=payload, headers=headers, timeout=15)
        
        print(f"[DEBUG] API Response Status: {response.status_code}")
        
        if response.status_code != 200:
            print(f"[!] OpenAI API hatasÄ± (Status: {response.status_code}): {response.text}")
            print(f"[+] Kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor: {selected_flair}")
            return selected_flair_id
        
        result = response.json()
        print(f"[DEBUG] OpenAI Response: {result}")
        
        # AI yanÄ±tÄ±nÄ± al
        if "choices" in result and len(result["choices"]) > 0:
            ai_suggestion = result["choices"][0]["message"]["content"].strip()
            print(f"[+] AI flair Ã¶nerisi: {ai_suggestion}")
            
            # Flair adÄ±nÄ± temizle ve kontrol et
            ai_suggestion_clean = ai_suggestion.replace(".", "").replace(":", "").strip()
            
            # Flair seÃ§eneklerinde ara
            for flair_name, flair_id in FLAIR_OPTIONS.items():
                if flair_name.lower() in ai_suggestion_clean.lower() or ai_suggestion_clean.lower() in flair_name.lower():
                    print(f"[+] AI seÃ§ilen flair: {flair_name} (ID: {flair_id})")
                    return flair_id
            
            # Tam eÅŸleÅŸme bulunamazsa, kural tabanlÄ± seÃ§imi kullan
            print(f"[!] AI Ã¶nerisi eÅŸleÅŸmedi ({ai_suggestion_clean}), kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor: {selected_flair}")
            return selected_flair_id
        else:
            print("[!] AI yanÄ±tÄ± alÄ±namadÄ±, kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor")
            return selected_flair_id
            
    except requests.exceptions.Timeout:
        print("[!] AI API timeout, kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor")
        return selected_flair_id
    except requests.exceptions.RequestException as req_e:
        print(f"[!] AI API Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z: {req_e}")
        print(f"[+] Kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor: {selected_flair}")
        return selected_flair_id
    except Exception as e:
        print(f"[!] Flair seÃ§imi hatasÄ±: {e}")
        print(f"[+] Kural tabanlÄ± seÃ§im kullanÄ±lÄ±yor: {selected_flair}")
        import traceback
        traceback.print_exc()
        return selected_flair_id

def download_media(media_url, filename):
    try:
        with requests.get(media_url, stream=True, timeout=30) as r:
            if r.status_code == 200:
                with open(filename, "wb") as f:
                    for chunk in r.iter_content(1024 * 64):
                        if chunk:
                            f.write(chunk)
                return filename
            else:
                print(f"[HATA] Medya indirilemedi: {media_url} - Status: {r.status_code}")
                return None
    except Exception as e:
        print(f"[HATA] Medya indirirken: {e}")
        return None

def get_image_hash(image_path):
    """Resim dosyasÄ±nÄ±n hash'ini hesapla (duplicate detection iÃ§in)"""
    try:
        hasher = hashlib.md5()
        with open(image_path, 'rb') as f:
            for chunk in iter(lambda: f.read(1024 * 1024), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    except Exception as e:
        print(f"[HATA] Hash hesaplanÄ±rken: {e}")
        return None

def download_multiple_images(media_urls, tweet_id):
    """Birden fazla resmi indir ve duplicate'leri filtrele"""
    downloaded_images = []
    image_hashes = set()
    
    print(f"[+] {len(media_urls)} medya URL'si iÅŸleniyor...")
    
    for i, media_url in enumerate(media_urls):
        try:
            # GeliÅŸtirilmiÅŸ resim tespiti
            url_lower = media_url.lower()
            is_image = (
                any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']) or
                'format=jpg' in url_lower or 'format=jpeg' in url_lower or 
                'format=png' in url_lower or 'format=webp' in url_lower or
                'pbs.twimg.com/media' in url_lower
            )
            if not is_image:
                print(f"[!] Resim olmayan medya atlanÄ±yor: {media_url}")
                continue
                
            ext = os.path.splitext(media_url)[1].split("?")[0]
            if not ext:
                ext = ".jpg"  # Default extension
            
            filename = f"temp_image_{tweet_id}_{i}{ext}"
            print(f"[+] Resim indiriliyor ({i+1}/{len(media_urls)}): {media_url[:50]}...")
            
            path = download_media(media_url, filename)
            if path and os.path.exists(path):
                # Hash kontrolÃ¼ ile duplicate detection
                image_hash = get_image_hash(path)
                if image_hash and image_hash not in image_hashes:
                    image_hashes.add(image_hash)
                    downloaded_images.append(path)
                    print(f"[+] Benzersiz resim eklendi: {path}")
                else:
                    print(f"[!] Duplicate resim atlandÄ±: {path}")
                    # Duplicate dosyayÄ± sil
                    if os.path.exists(path):
                        os.remove(path)
            else:
                print(f"[!] Resim indirilemedi: {media_url}")
                
        except Exception as e:
            print(f"[HATA] Resim iÅŸleme hatasÄ± ({media_url}): {e}")
    
    print(f"[+] Toplam {len(downloaded_images)} benzersiz resim indirildi")
    return downloaded_images

def convert_video_to_reddit_format(input_path, output_path):
    """Reddit iÃ§in optimize edilmiÅŸ video dÃ¶nÃ¼ÅŸtÃ¼rme"""
    try:
        print(f"[+] Reddit uyumlu video dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸlatÄ±lÄ±yor: {input_path} -> {output_path}")
        
        # Video bilgilerini kontrol et
        probe_cmd = ["ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", "-show_streams", input_path]
        try:
            probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, check=True)
            video_info = json.loads(probe_result.stdout)
            
            duration = float(video_info['format']['duration'])
            if duration > 900:  # 15 dakika Reddit limiti
                print(f"[HATA] Video Ã§ok uzun ({duration:.1f}s). Reddit limiti: 900s")
                return None
                
            print(f"[+] Orijinal video sÃ¼resi: {duration:.1f}s")
            
        except Exception as probe_e:
            print(f"[UYARI] Video bilgisi alÄ±namadÄ±: {probe_e}")
            # SÃ¼re bilinmiyorsa makul bir sÃ¼re kullan
            duration = 120.0
        
        # OPTIMIZE EDÄ°LMÄ°Åž FFmpeg komutu - 4K video ve bellek sorunlarÄ± iÃ§in
        cmd = [
            "ffmpeg",
            "-i", input_path,
            "-c:v", "libx264",
            "-profile:v", "baseline",  # En uyumlu profil
            "-level", "3.1",  # Daha dÃ¼ÅŸÃ¼k level (daha az bellek)
            "-preset", "veryfast",  # Daha hÄ±zlÄ± iÅŸlem iÃ§in
            "-crf", "28",  # Daha yÃ¼ksek CRF (kÃ¼Ã§Ã¼k dosya)
            "-maxrate", "2M",  # DÃ¼ÅŸÃ¼k bitrate
            "-bufsize", "4M",  # KÃ¼Ã§Ã¼k buffer
            "-g", "30",
            "-keyint_min", "30",
            "-sc_threshold", "0",
            "-c:a", "aac",
            "-b:a", "96k",  # DÃ¼ÅŸÃ¼k audio bitrate
            "-ar", "44100",
            "-ac", "2",
            "-movflags", "+faststart",
            "-pix_fmt", "yuv420p",
            "-vf", "scale='min(1280,iw)':'min(720,ih)':force_original_aspect_ratio=decrease:flags=fast_bilinear,pad=ceil(iw/2)*2:ceil(ih/2)*2,fps=24",  # 720p max, 24fps
            "-r", "24",  # DÃ¼ÅŸÃ¼k framerate
            "-avoid_negative_ts", "make_zero",
            "-fflags", "+genpts",
            "-map_metadata", "-1",
            "-threads", "1",  # Daha az thread (daha dÃ¼ÅŸÃ¼k bellek kullanÄ±mÄ±)
            "-y",
            output_path
        ]
        
        print(f"[+] Reddit uyumlu FFmpeg komutu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        print(f"[DEBUG] Komut: {' '.join(cmd[:10])}...")  # Ä°lk 10 parametreyi gÃ¶ster
        # SÃ¼reye gÃ¶re uyarlanabilir timeout (min 5dk, max 15dk)
        conv_timeout = int(max(300, min(900, duration * 8)))
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=conv_timeout)
        
        if result.returncode != 0:
            print(f"[HATA] FFmpeg baÅŸarÄ±sÄ±z (code: {result.returncode})")
            print(f"[HATA] FFmpeg stderr: {result.stderr[:500]}")  # Ä°lk 500 karakter
            return None
        
        # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ dosyayÄ± kontrol et
        if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:  # En az 1KB
            file_size = os.path.getsize(output_path)
            print(f"[+] Video baÅŸarÄ±yla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼: {output_path} ({file_size} bytes)")
            
            # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ video bilgilerini kontrol et
            try:
                probe_result2 = subprocess.run(probe_cmd[:-1] + [output_path], capture_output=True, text=True, check=True)
                video_info2 = json.loads(probe_result2.stdout)
                
                video_streams = [s for s in video_info2.get('streams', []) if s.get('codec_type') == 'video']
                if video_streams:
                    codec = video_streams[0].get('codec_name', 'unknown')
                    width = video_streams[0].get('width', 0)
                    height = video_streams[0].get('height', 0)
                    print(f"[+] DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ video: {codec}, {width}x{height}")
                    
            except Exception as probe_e2:
                print(f"[UYARI] DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ video bilgisi alÄ±namadÄ±: {probe_e2}")
            
            return output_path
        else:
            print("[HATA] DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ video dosyasÄ± geÃ§ersiz")
            return None
            
    except subprocess.TimeoutExpired:
        print("[HATA] FFmpeg timeout - bir fallback ile yeniden denenecek")
        try:
            # Daha agresif: ultrafast preset, biraz daha dÃ¼ÅŸÃ¼k bitrate, aynÄ± 720p/24fps
            fallback_cmd = [
                "ffmpeg",
                "-i", input_path,
                "-c:v", "libx264",
                "-profile:v", "baseline",
                "-level", "3.1",
                "-preset", "ultrafast",
                "-crf", "30",
                "-maxrate", "1.5M",
                "-bufsize", "3M",
                "-g", "30",
                "-keyint_min", "30",
                "-sc_threshold", "0",
                "-c:a", "aac",
                "-b:a", "96k",
                "-ar", "44100",
                "-ac", "2",
                "-movflags", "+faststart",
                "-pix_fmt", "yuv420p",
                "-vf", "scale='min(1280,iw)':'min(720,ih)':force_original_aspect_ratio=decrease:flags=fast_bilinear,pad=ceil(iw/2)*2:ceil(ih/2)*2,fps=24",
                "-r", "24",
                "-avoid_negative_ts", "make_zero",
                "-fflags", "+genpts",
                "-map_metadata", "-1",
                "-threads", "2",
                "-y",
                output_path
            ]
            print("[+] Fallback FFmpeg komutu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor (ultrafast)...")
            fb_timeout = 900  # 15 dakika son ÅŸans
            fb_res = subprocess.run(fallback_cmd, capture_output=True, text=True, timeout=fb_timeout)
            if fb_res.returncode == 0 and os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                print("[+] Fallback dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±lÄ±")
                return output_path
            else:
                print(f"[HATA] Fallback FFmpeg baÅŸarÄ±sÄ±z (code: {fb_res.returncode})")
                print(f"[HATA] Fallback stderr: {fb_res.stderr[:500]}")
                return None
        except subprocess.TimeoutExpired:
            print("[HATA] Fallback FFmpeg de timeout verdi")
            return None
    except Exception as e:
        print(f"[HATA] Video dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: {e}")
        return None

def get_video_duration_seconds(path: str) -> float | None:
    """ffprobe ile video sÃ¼resini saniye cinsinden dÃ¶ndÃ¼r. Hata halinde None dÃ¶ner."""
    try:
        cmd = ["ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", path]
        res = subprocess.run(cmd, capture_output=True, text=True, check=True)
        info = json.loads(res.stdout or "{}")
        dur = float(info.get("format", {}).get("duration", "nan"))
        if dur != dur:  # NaN kontrolÃ¼
            return None
        return dur
    except Exception as e:
        print(f"[UYARI] Video sÃ¼resi alÄ±namadÄ±: {e}")
        return None

# --- Scheduled weekly "Oyuncu Arama" post helper (stateless) ---
def _create_and_pin_weekly_post_if_due() -> None:
    """Scheduled weekly megathread creator.
    Primary schedule: every Friday (weekday=4) at/after configured hour.
    Fallback schedule: specified days-of-month in `SCHEDULED_PIN_DAYS` if WEEKDAY invalid.
    Uses PRAW and requires moderator permissions on the subreddit.
    """
    try:
        # Global toggle
        if not SCHEDULED_PIN_ENABLED:
            return
        lt = time.localtime()
        day = lt.tm_mday
        wday = lt.tm_wday  # 0=Mon .. 6=Sun
        hour = lt.tm_hour

        run_today = False
        # Prefer weekday schedule if valid
        if 0 <= SCHEDULED_PIN_WEEKDAY <= 6:
            run_today = (wday == SCHEDULED_PIN_WEEKDAY) and (hour >= SCHEDULED_PIN_HOUR)
        else:
            # Fallback to day-of-month list if provided
            if SCHEDULED_PIN_DAYS:
                run_today = (day in SCHEDULED_PIN_DAYS) and (hour >= SCHEDULED_PIN_HOUR)

        if not run_today:
            return
        today_key = time.strftime("%Y-%m-%d", lt)

        # Build title and body
        tarih = time.strftime("%d.%m.%Y", lt)
        title = f"{SCHEDULED_PIN_TITLE_PREFIX}{tarih})"
        body = (
            "**HoÅŸ geldiniz!**\n\n"
            "Bu baÅŸlÄ±k, takÄ±m/oyuncu bulmanÄ±zÄ± kolaylaÅŸtÄ±rmak amacÄ±yla oluÅŸturulmuÅŸtur. EÄŸer birlikte oyun oynayabileceÄŸiniz yeni kiÅŸiler arÄ±yorsanÄ±z doÄŸru yerdesiniz! AÅŸaÄŸÄ±da belirtildiÄŸi ÅŸekilde yorum yaparak takÄ±m arkadaÅŸÄ± arayabilirsiniz. BÃ¶ylece benzer oyunlara ve tercihlere sahip oyuncular kolayca bir araya gelebilir.\n\n"
            "**LÃ¼tfen yorumlarÄ±nÄ±zda ÅŸunlarÄ± belirtmeyi unutmayÄ±n:**\n\n"
            "* Oyun platformunuz (PC, PlayStation, Xbox vb.)\n"
            "* Oyun iÃ§i kullanÄ±cÄ± adÄ±nÄ±z\n"
            "* Mikrofonlu/suz bilgisi\n"
            "* Genellikle oynadÄ±ÄŸÄ±nÄ±z veya oynayacaÄŸÄ±nÄ±z gÃ¶rev birimi (assault, medic, recon vb.)\n\n"
            "Bu bilgiler sayesinde, benzer oyun ve oyun tarzlarÄ±na sahip kiÅŸilerle daha kolay iletiÅŸim kurabilirsiniz.\n\n"
            "**YorumlarÄ±n sÄ±ralanmasÄ±:**\n\n"
            "Yorumlar sistem tarafÄ±ndan otomatik olarak en yeni yorumdan en eski yoruma doÄŸru sÄ±ralanmaktadÄ±r. BÃ¶ylece en gÃ¼ncel oyunculara ve taleplere kolayca ulaÅŸabilirsiniz.\n\n"
            "Her seviyeden oyuncuya aÃ§Ä±ktÄ±r, saygÄ±lÄ± ve destekleyici bir ortam yaratmayÄ± amaÃ§lÄ±yoruz. Keyifli oyunlar!"
        )

        # Stateless dedupe: scan recent posts for today's weekly thread
        sr = reddit.subreddit(SUBREDDIT)
        try:
            for s in sr.new(limit=30):
                try:
                    author_name = getattr(s.author, 'name', '') or ''
                except Exception:
                    author_name = ''
                s_title = getattr(s, 'title', '') or ''
                try:
                    created_utc = int(getattr(s, 'created_utc', 0) or 0)
                except Exception:
                    created_utc = 0
                s_local = time.localtime(created_utc) if created_utc else None
                s_key = time.strftime("%Y-%m-%d", s_local) if s_local else ''
                if (
                    author_name.lower() == (REDDIT_USERNAME or '').lower()
                    and s_title.startswith(SCHEDULED_PIN_TITLE_PREFIX)
                    and s_key == today_key
                ):
                    print(f"[INFO] BugÃ¼nÃ¼n haftalÄ±k baÅŸlÄ±ÄŸÄ± zaten mevcut: https://reddit.com{s.permalink}")
                    return
        except Exception as scan_e:
            print(f"[UYARI] Mevcut haftalÄ±k gÃ¶nderiler taranamadÄ±: {scan_e}")

        print("[+] HaftalÄ±k oyuncu arama gÃ¶nderisi oluÅŸturuluyor ve sabitleniyor...")
        submission = sr.submit(title=title, selftext=body, send_replies=False, resubmit=False)
        if submission:
            try:
                # Pin to top (slot 1) and set suggested sort to 'new'
                submission.mod.sticky(state=True, bottom=False)
                try:
                    submission.mod.suggested_sort("new")
                except Exception as se:
                    print(f"[UYARI] suggested_sort ayarlanamadÄ±: {se}")
                print(f"[+] HaftalÄ±k gÃ¶nderi oluÅŸturuldu ve sabitlendi: https://reddit.com{submission.permalink}")

                # Unsticky older megathreads so only the newest stays pinned
                try:
                    for slot in (1, 2):
                        try:
                            stickied = sr.sticky(number=slot)
                        except Exception:
                            stickied = None
                        if not stickied:
                            continue
                        try:
                            st_author = getattr(stickied.author, 'name', '') or ''
                        except Exception:
                            st_author = ''
                        st_title = getattr(stickied, 'title', '') or ''
                        if (
                            stickied.id != submission.id and
                            st_author.lower() == (REDDIT_USERNAME or '').lower() and
                            st_title.startswith(SCHEDULED_PIN_TITLE_PREFIX)
                        ):
                            try:
                                stickied.mod.sticky(state=False)
                                print(f"[+] Eski haftalÄ±k baÅŸlÄ±k unsticky yapÄ±ldÄ±: https://reddit.com{stickied.permalink}")
                            except Exception as ue:
                                print(f"[UYARI] Unsticky baÅŸarÄ±sÄ±z: {ue}")
                except Exception as sweep_e:
                    print(f"[UYARI] Sticky temizleme sÄ±rasÄ±nda hata: {sweep_e}")
                # Extra sweep: scan recent posts and unsticky any lingering older stickied megathreads
                try:
                    for s in sr.new(limit=100):
                        try:
                            if getattr(s, 'id', None) == submission.id:
                                continue
                            if not getattr(s, 'stickied', False):
                                continue
                            author_name = getattr(s.author, 'name', '') or ''
                            s_title = getattr(s, 'title', '') or ''
                            if (
                                author_name.lower() == (REDDIT_USERNAME or '').lower() and
                                s_title.startswith(SCHEDULED_PIN_TITLE_PREFIX)
                            ):
                                try:
                                    s.mod.sticky(state=False)
                                    print(f"[+] Eski stickied gÃ¶nderi unsticky yapÄ±ldÄ±: https://reddit.com{s.permalink}")
                                except Exception as ue2:
                                    print(f"[UYARI] Unsticky (scan) baÅŸarÄ±sÄ±z: {ue2}")
                        except Exception:
                            continue
                except Exception as scan_uns_e:
                    print(f"[UYARI] Ek sticky tarama/temizleme hatasÄ±: {scan_uns_e}")
            except Exception as me:
                print(f"[UYARI] GÃ¶nderi sabitleme/moderasyon iÅŸlemi baÅŸarÄ±sÄ±z: {me}")
        else:
            print("[UYARI] HaftalÄ±k gÃ¶nderi oluÅŸturulamadÄ± (PRAW submit falsy)")
    except Exception as e:
        print(f"[UYARI] HaftalÄ±k gÃ¶nderi oluÅŸturma hatasÄ±: {e}")
def upload_gallery_via_redditwarp(title, image_paths, subreddit_name, flair_id=None):
    """RedditWarp ile birden fazla resmi gallery olarak yÃ¼kle"""
    if not redditwarp_client:
        print("[HATA] RedditWarp client mevcut deÄŸil")
        return False
        
    if not image_paths:
        print("[HATA] YÃ¼klenecek resim yok")
        return False
        
    try:
        print(f"[+] {len(image_paths)} resim iÃ§in gallery oluÅŸturuluyor...")
        
        # GÃ¶rselleri normalize et (RGB, boyut sÄ±nÄ±rÄ±, baseline JPEG)
        def _normalize_image(path):
            try:
                with Image.open(path) as im:
                    im = im.convert("RGB")
                    max_dim = 4096
                    w, h = im.size
                    if max(w, h) > max_dim:
                        if w >= h:
                            new_w = max_dim
                            new_h = int(h * (max_dim / w))
                        else:
                            new_h = max_dim
                            new_w = int(w * (max_dim / h))
                        im = im.resize((new_w, new_h), Image.Resampling.LANCZOS)
                    out_path = os.path.join(os.path.dirname(path), os.path.splitext(os.path.basename(path))[0] + "_norm.jpg")
                    im.save(out_path, format="JPEG", quality=90, optimize=True, progressive=False)
                    print(f"[+] Normalize edildi: {os.path.basename(out_path)}")
                    return out_path
            except Exception as _norm_e:
                print(f"[UYARI] Normalize edilemedi ({os.path.basename(path)}): {_norm_e}")
            return path
        
        norm_image_paths = [_normalize_image(p) for p in image_paths]
        
        # Her resim iÃ§in upload lease al
        image_leases = []
        for i, image_path in enumerate(norm_image_paths):
            print(f"[+] Resim {i+1}/{len(image_paths)} yÃ¼kleniyor: {os.path.basename(image_path)}")
            
            if not os.path.exists(image_path):
                print(f"[HATA] Resim dosyasÄ± bulunamadÄ±: {image_path}")
                continue
                
            try:
                with open(image_path, 'rb') as image_file:
                    # RedditWarp ile resim upload
                    image_lease = redditwarp_client.p.submission.media_uploading.upload(image_file)
                    image_leases.append(image_lease)
                    print(f"[+] Resim lease alÄ±ndÄ± - Media ID: {image_lease.media_id}")
                
            except Exception as upload_e:
                print(f"[HATA] Resim yÃ¼kleme hatasÄ± ({image_path}): {upload_e}")
                continue
        
        if not image_leases:
            print("[HATA] HiÃ§bir resim yÃ¼klenemedi")
            return False
            
        print(f"[+] {len(image_leases)} resim baÅŸarÄ±yla yÃ¼klendi, gallery oluÅŸturuluyor...")
        
        # Gallery post oluÅŸtur
        try:
            # RedditWarp gallery creation
            gallery_items = [
                SimpleNamespace(
                    id=idx,
                    media_id=lease.media_id,
                    caption=f"Resim {idx+1}",
                    outbound_link="",
                )
                for idx, lease in enumerate(image_leases)
            ]
            
            created = redditwarp_client.p.submission.create.gallery(
                sr=subreddit_name,
                title=title,
                items=gallery_items
            )
            
            if created:
                submission_id = getattr(created, 'id', str(created))
                print(f"[+] Gallery baÅŸarÄ±yla oluÅŸturuldu - ID: {submission_id}")
                # Flair uygula (mÃ¼mkÃ¼nse)
                try:
                    # Try to resolve flair per subreddit before applying
                    effective_flair_id = _resolve_flair_id_for_subreddit(subreddit_name, flair_id, _FLAIR_NAME_BY_ID.get(flair_id)) if flair_id else None
                    if effective_flair_id and submission_id and isinstance(submission_id, str):
                        try:
                            praw_sub = reddit.submission(id=submission_id)
                            praw_sub.flair.select(effective_flair_id)
                            print(f"[+] Gallery flair uygulandÄ±: {effective_flair_id}")
                        except Exception as fe:
                            print(f"[UYARI] Gallery flair uygulanamadÄ±: {fe}")
                    elif effective_flair_id:
                        # BaÅŸlÄ±ÄŸa gÃ¶re yakÄ±n zamanda oluÅŸturulan gÃ¶nderiyi bul
                        try:
                            sr_obj = reddit.subreddit(subreddit_name)
                            for s in sr_obj.new(limit=10):
                                author_name = getattr(s.author, 'name', '') or ''
                                if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                                    s.flair.select(effective_flair_id)
                                    print(f"[+] Gallery flair uygulandÄ± (arama ile): {effective_flair_id}")
                                    break
                        except Exception as fe2:
                            print(f"[UYARI] Gallery flair uygulanamadÄ± (arama): {fe2}")
                except Exception:
                    pass
                # BaÅŸarÄ±lÄ± gÃ¶nderiden sonra disk temizliÄŸi
                try:
                    # Normalized dosyalarÄ± sil
                    for p in norm_image_paths:
                        if p and os.path.exists(p):
                            try:
                                os.remove(p)
                                print(f"[TEMÄ°ZLÄ°K] Silindi: {p}")
                            except Exception as de:
                                print(f"[UYARI] Silinemedi: {p} - {de}")
                    # Orijinal dosyalarÄ± da sil
                    for p in image_paths:
                        if p and os.path.exists(p):
                            try:
                                os.remove(p)
                                print(f"[TEMÄ°ZLÄ°K] Silindi: {p}")
                            except Exception as de:
                                print(f"[UYARI] Silinemedi: {p} - {de}")
                except Exception as ce:
                    print(f"[UYARI] Temizlik sÄ±rasÄ±nda hata: {ce}")
                return True
            else:
                # RedditWarp bazen creation nesnesini dÃ¶ndÃ¼rmeyebilir; canlÄ±da doÄŸrula
                print("[UYARI] Gallery API yanÄ±tÄ± falsy gÃ¶rÃ¼nÃ¼yor, subreddit'te doÄŸrulanÄ±yor...")
                try:
                    sr_obj = reddit.subreddit(subreddit_name)
                    for s in sr_obj.new(limit=10):
                        author_name = getattr(s.author, 'name', '') or ''
                        if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                            print(f"[+] Gallery gÃ¶nderisi doÄŸrulandÄ±: {s.url}")
                            # Flair uygula
                            try:
                                if flair_id:
                                    efid = _resolve_flair_id_for_subreddit(subreddit_name, flair_id, _FLAIR_NAME_BY_ID.get(flair_id))
                                    if efid:
                                        s.flair.select(efid)
                                        print(f"[+] Gallery flair uygulandÄ± (doÄŸrulama): {efid}")
                            except Exception as fe:
                                print(f"[UYARI] Gallery flair uygulanamadÄ± (doÄŸrulama): {fe}")
                            # BaÅŸarÄ±lÄ± kabul et ve dosyalarÄ± temizle
                            try:
                                for p in norm_image_paths:
                                    if p and os.path.exists(p):
                                        try:
                                            os.remove(p)
                                            print(f"[TEMÄ°ZLÄ°K] Silindi: {p}")
                                        except Exception as de:
                                            print(f"[UYARI] Silinemedi: {p} - {de}")
                                for p in image_paths:
                                    if p and os.path.exists(p):
                                        try:
                                            os.remove(p)
                                            print(f"[TEMÄ°ZLÄ°K] Silindi: {p}")
                                        except Exception as de:
                                            print(f"[UYARI] Silinemedi: {p} - {de}")
                            except Exception as ce:
                                print(f"[UYARI] Temizlik sÄ±rasÄ±nda hata: {ce}")
                            return True
                except Exception as ve:
                    print(f"[UYARI] Gallery doÄŸrulama hatasÄ±: {ve}")
                print("[HATA] Gallery oluÅŸturulamadÄ±")
                return False
        except Exception as create_e:
            print(f"[HATA] Gallery oluÅŸturma hatasÄ±: {create_e}")
            # Hata durumunda da doÄŸrulamayÄ± dene (async yaratÄ±lmÄ±ÅŸ olabilir)
            try:
                sr_obj = reddit.subreddit(subreddit_name)
                for s in sr_obj.new(limit=10):
                    author_name = getattr(s.author, 'name', '') or ''
                    if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                        print(f"[+] Gallery gÃ¶nderisi hata sonrasÄ±nda doÄŸrulandÄ±: {s.url}")
                        # Flair uygula
                        try:
                            if flair_id:
                                efid = _resolve_flair_id_for_subreddit(subreddit_name, flair_id, _FLAIR_NAME_BY_ID.get(flair_id))
                                if efid:
                                    s.flair.select(efid)
                                    print(f"[+] Gallery flair uygulandÄ± (hata sonrasÄ± doÄŸrulama): {efid}")
                        except Exception as fe:
                            print(f"[UYARI] Gallery flair uygulanamadÄ± (hata sonrasÄ± doÄŸrulama): {fe}")
                        # BaÅŸarÄ±lÄ± kabul et ve dosyalarÄ± temizle
                        try:
                            for p in norm_image_paths:
                                if p and os.path.exists(p):
                                    try:
                                        os.remove(p)
                                        print(f"[TEMÄ°ZLÄ°K] Silindi: {p}")
                                    except Exception as de:
                                        print(f"[UYARI] Silinemedi: {p} - {de}")
                            for p in image_paths:
                                if p and os.path.exists(p):
                                    try:
                                        os.remove(p)
                                        print(f"[TEMÄ°ZLÄ°K] Silindi: {p}")
                                    except Exception as de:
                                        print(f"[UYARI] Silinemedi: {p} - {de}")
                        except Exception as ce:
                            print(f"[UYARI] Temizlik sÄ±rasÄ±nda hata: {ce}")
                        return True
            except Exception as ve:
                print(f"[UYARI] Gallery doÄŸrulama hatasÄ±: {ve}")
            return False
        
    except Exception as e:
        print(f"[HATA] RedditWarp gallery yÃ¼kleme genel hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return False

def upload_video_via_redditwarp(title, media_path, subreddit_name, flair_id=None):
    """RedditWarp dokÃ¼mantasyonuna gÃ¶re video yÃ¼kleme - Media Upload Protocol"""
    if not redditwarp_client:
        print("[HATA] RedditWarp client mevcut deÄŸil")
        return False
        
    try:
        print("[+] RedditWarp ile video yÃ¼kleme baÅŸlatÄ±lÄ±yor...")
        
        # Dosya kontrolleri
        if not os.path.exists(media_path):
            print(f"[HATA] Video dosyasÄ± bulunamadÄ±: {media_path}")
            return False
            
        file_size = os.path.getsize(media_path)
        if file_size == 0:
            print(f"[HATA] Video dosyasÄ± boÅŸ: {media_path}")
            return False
            
        # Reddit limitleri - dokÃ¼mantasyona gÃ¶re
        max_size = 1024 * 1024 * 1024  # 1GB
        if file_size > max_size:
            print(f"[HATA] Video Ã§ok bÃ¼yÃ¼k ({file_size} bytes). Reddit limiti: {max_size} bytes")
            return False
            
        print(f"[+] Video dosyasÄ± geÃ§erli: {file_size} bytes")
        
        # Video bilgilerini doÄŸrula
        try:
            probe_cmd = ["ffprobe", "-v", "quiet", "-print_format", "json", "-show_format", "-show_streams", media_path]
            probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, check=True)
            video_info = json.loads(probe_result.stdout)
            
            duration = float(video_info['format']['duration'])
            if duration > 900:  # 15 dakika - dokÃ¼mantasyona gÃ¶re
                print(f"[HATA] Video Ã§ok uzun ({duration:.1f}s). Reddit limiti: 900s")
                return False
            
            # Video stream kontrolÃ¼
            video_streams = [s for s in video_info.get('streams', []) if s.get('codec_type') == 'video']
            if not video_streams:
                print("[HATA] Video stream bulunamadÄ±")
                return False
                
            codec = video_streams[0].get('codec_name', '')
            print(f"[+] Video geÃ§erli - Codec: {codec}, SÃ¼re: {duration:.1f}s")
            
        except Exception as probe_e:
            print(f"[HATA] Video doÄŸrulama baÅŸarÄ±sÄ±z: {probe_e}")
            return False
        
        # Rate limiting
        print("[+] Rate limit iÃ§in 3 saniye bekleniyor...")
        time.sleep(3)
        
        # RedditWarp Media Upload Protocol - dokÃ¼mantasyona gÃ¶re 2 adÄ±mlÄ± sÃ¼reÃ§
        print("[+] RedditWarp Media Upload Protocol baÅŸlatÄ±lÄ±yor...")
        
        # AdÄ±m 1: Video iÃ§in upload lease al
        print("[+] Video upload lease alÄ±nÄ±yor...")
        try:
            filename = os.path.basename(media_path)
            with open(media_path, 'rb') as video_file:
                # RedditWarp dokÃ¼mantasyonuna gÃ¶re: submission.media_uploading.upload()
                video_lease = redditwarp_client.p.submission.media_uploading.upload(video_file)
                print(f"[+] Video lease alÄ±ndÄ± - Media ID: {video_lease.media_id}")
                print(f"[+] Video S3 Location: {video_lease.location}")
                
        except Exception as video_lease_e:
            print(f"[HATA] Video upload lease hatasÄ±: {video_lease_e}")
            return False
        
        # Thumbnail oluÅŸtur (RedditWarp dokÃ¼mantasyonu video post iÃ§in thumbnail gerektirir)
        print("[+] Video thumbnail oluÅŸturuluyor...")
        thumbnail_path = None
        try:
            # FFmpeg ile thumbnail oluÅŸtur
            thumbnail_filename = f"thumb_{os.path.splitext(filename)[0]}.jpg"
            thumbnail_path = os.path.join(os.path.dirname(media_path), thumbnail_filename)
            
            thumb_cmd = [
                "ffmpeg", "-i", media_path, "-ss", "00:00:01", "-vframes", "1",
                "-vf", "scale=640:360:force_original_aspect_ratio=decrease",
                "-y", thumbnail_path
            ]
            
            subprocess.run(thumb_cmd, capture_output=True, check=True)
            
            if os.path.exists(thumbnail_path) and os.path.getsize(thumbnail_path) > 0:
                print(f"[+] Thumbnail oluÅŸturuldu: {thumbnail_path}")
                
                # AdÄ±m 2: Thumbnail iÃ§in upload lease al
                print("[+] Thumbnail upload lease alÄ±nÄ±yor...")
                with open(thumbnail_path, 'rb') as thumb_file:
                    thumb_lease = redditwarp_client.p.submission.media_uploading.upload(thumb_file)
                    print(f"[+] Thumbnail lease alÄ±ndÄ± - Media ID: {thumb_lease.media_id}")
                    print(f"[+] Thumbnail S3 Location: {thumb_lease.location}")
            else:
                print("[UYARI] Thumbnail oluÅŸturulamadÄ±, video olmadan devam ediliyor")
                thumb_lease = None
                
        except Exception as thumb_e:
            print(f"[UYARI] Thumbnail oluÅŸturma hatasÄ±: {thumb_e}")
            thumb_lease = None
            if thumbnail_path and os.path.exists(thumbnail_path):
                try:
                    os.remove(thumbnail_path)
                except:
                    pass
        
        # AdÄ±m 3: Video post oluÅŸtur - RedditWarp dokÃ¼mantasyonuna gÃ¶re
        print("[+] Video submission oluÅŸturuluyor...")
        try:
            if thumb_lease:
                # Thumbnail ile video post - doÄŸru RedditWarp metodu ve parametreler
                created = redditwarp_client.p.submission.create.video(
                    sr=subreddit_name,
                    title=title,
                    link=video_lease.location,
                    thumbnail=thumb_lease.location
                )
            else:
                # Sadece video ile post (thumbnail olmadan)
                # RedditWarp dokÃ¼mantasyonuna gÃ¶re thumbnail gerekli, bu durumda hata verebilir
                created = redditwarp_client.p.submission.create.video(
                    sr=subreddit_name,
                    title=title,
                    link=video_lease.location,
                    thumbnail=video_lease.location  # Thumbnail yerine video location kullan
                )
            
            # If we reach here without exception, the submission was successful
            print("[+] RedditWarp video submission baÅŸarÄ±lÄ±!")
            
            # OluÅŸan gÃ¶nderinin ID'sini elde etmeyi dene
            submission_id = None
            try:
                # OlasÄ± alan adlarÄ±: id / id36 / post_id / submission_id / fullname (t3_xxxxx)
                if isinstance(created, str):
                    # BazÄ± sÃ¼rÃ¼mler fullname (t3_xxxxx) dÃ¶ndÃ¼rebilir
                    if created.startswith("t3_"):
                        submission_id = created.split("t3_")[-1]
                    else:
                        submission_id = created
                elif hasattr(created, "id"):
                    submission_id = getattr(created, "id")
                elif hasattr(created, "id36"):
                    submission_id = getattr(created, "id36")
                elif hasattr(created, "submission_id"):
                    submission_id = getattr(created, "submission_id")
                elif hasattr(created, "post_id"):
                    submission_id = getattr(created, "post_id")
                elif hasattr(created, "fullname"):
                    fullname = getattr(created, "fullname")
                    if isinstance(fullname, str) and fullname.startswith("t3_"):
                        submission_id = fullname.split("t3_")[-1]
                elif hasattr(created, "get"):
                    # dict benzeri
                    submission_id = created.get("id") or created.get("id36") or created.get("submission_id") or created.get("post_id")
            except Exception as id_e:
                print(f"[UYARI] RedditWarp dÃ¶nen nesneden ID Ã§Ä±karÄ±lamadÄ±: {id_e}")

            if submission_id:
                print(f"[+] OluÅŸan gÃ¶nderi ID: {submission_id}")
                print(f"[+] URL: https://reddit.com/r/{subreddit_name}/comments/{submission_id}")
                # Flair uygula (ID mevcutsa)
                try:
                    if flair_id:
                        efid = _resolve_flair_id_for_subreddit(subreddit_name, flair_id, _FLAIR_NAME_BY_ID.get(flair_id))
                        if efid:
                            praw_sub = reddit.submission(id=submission_id)
                            praw_sub.flair.select(efid)
                            print(f"[+] Video flair uygulandÄ± (ID ile): {efid}")
                except Exception as fe:
                    print(f"[UYARI] Video flair uygulanamadÄ± (ID ile): {fe}")
            else:
                print("[UYARI] RedditWarp oluÅŸturulan gÃ¶nderi ID'si alÄ±namadÄ±")
                submission_id = ""
                
            # Video processing bekle
            print("[+] Video processing iÃ§in 30 saniye bekleniyor...")
            time.sleep(30)
            
            # Flair yoksa, baÅŸlÄ±ÄŸa gÃ¶re en son gÃ¶nderiyi bulup uygula
            if (not submission_id) and flair_id:
                try:
                    efid = _resolve_flair_id_for_subreddit(subreddit_name, flair_id, _FLAIR_NAME_BY_ID.get(flair_id)) if flair_id else None
                    if efid:
                        sr_obj = reddit.subreddit(subreddit_name)
                        for s in sr_obj.new(limit=10):
                            author_name = getattr(s.author, 'name', '') or ''
                            if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                                s.flair.select(efid)
                                print(f"[+] Video flair uygulandÄ± (arama ile): {efid}")
                                break
                except Exception as fe:
                    print(f"[UYARI] Video flair uygulanamadÄ± (arama ile): {fe}")

            # Deterministik yorum iÃ§in ID dÃ¶ndÃ¼r
            return submission_id or True
                
        except Exception as submission_e:
            print(f"[HATA] Video submission hatasÄ±: {submission_e}")
            print(f"[DEBUG] Hata tipi: {type(submission_e).__name__}")
            
            # RedditError'u Ã¶zel olarak handle et
            if hasattr(submission_e, 'label'):
                print(f"[DEBUG] Reddit Error Label: {submission_e.label}")
                if hasattr(submission_e, 'explanation'):
                    print(f"[DEBUG] Reddit Error Explanation: {submission_e.explanation}")
                    
                # Spesifik Reddit hatalarÄ±
                if submission_e.label == 'NO_VIDEOS':
                    print("[!] Subreddit video post'lara izin vermiyor")
                elif submission_e.label == 'MISSING_VIDEO_URLS':
                    print("[!] Video URL'leri eksik veya geÃ§ersiz")
                elif submission_e.label == 'SUBREDDIT_NOTALLOWED':
                    print("[!] Subreddit'e post atma izni yok")
                elif submission_e.label == 'USER_REQUIRED':
                    print("[!] KullanÄ±cÄ± kimlik doÄŸrulama gerekli")
            
            # Genel hata tÃ¼rÃ¼ne gÃ¶re mesaj
            error_str = str(submission_e).lower()
            if "rate limit" in error_str or "429" in error_str:
                print("[!] Rate limit hatasÄ± - daha uzun bekleme gerekli")
            elif "413" in error_str or "too large" in error_str:
                print("[!] Dosya Ã§ok bÃ¼yÃ¼k hatasÄ±")
            elif "400" in error_str or "invalid" in error_str:
                print("[!] GeÃ§ersiz video formatÄ± veya parametreler")
            elif "403" in error_str or "forbidden" in error_str:
                print("[!] Ä°zin hatasÄ± - subreddit veya kullanÄ±cÄ± yetkileri")
            elif "timeout" in error_str:
                print("[!] Timeout hatasÄ± - video Ã§ok bÃ¼yÃ¼k veya aÄŸ yavaÅŸ")
            
            # Tam traceback'i gÃ¶ster
            import traceback
            print("[DEBUG] Tam hata detayÄ±:")
            traceback.print_exc()
            
            return False
            
    except Exception as e:
        print(f"[HATA] RedditWarp video yÃ¼kleme genel hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        # Thumbnail temizliÄŸi
        if thumbnail_path and os.path.exists(thumbnail_path):
            try:
                os.remove(thumbnail_path)
                print(f"[+] Thumbnail temizlendi: {thumbnail_path}")
            except Exception as cleanup_e:
                print(f"[UYARI] Thumbnail temizleme hatasÄ±: {cleanup_e}")

def upload_video_via_reddit_api(title, media_path, subreddit_name, flair_id=None):
    """Video yÃ¼kleme - Ã¶nce RedditWarp, sonra PRAW fallback"""
    
    # Ã–nce RedditWarp dene
    if redditwarp_client:
        print("[+] RedditWarp yÃ¶ntemi deneniyor...")
        warp_result = upload_video_via_redditwarp(title, media_path, subreddit_name, flair_id=flair_id)
        if warp_result:
            # ID string dÃ¶nebilir veya True olabilir
            return warp_result
        else:
            print("[!] RedditWarp baÅŸarÄ±sÄ±z, PRAW fallback deneniyor...")
    
    # PRAW fallback
    try:
        print("[+] PRAW fallback ile video yÃ¼kleme...")
        
        # Dosya kontrolleri
        if not os.path.exists(media_path):
            print(f"[HATA] Video dosyasÄ± bulunamadÄ±: {media_path}")
            return False
            
        file_size = os.path.getsize(media_path)
        if file_size == 0:
            print(f"[HATA] Video dosyasÄ± boÅŸ: {media_path}")
            return False
            
        print(f"[+] Video dosyasÄ± geÃ§erli: {file_size} bytes")
        
        subreddit = reddit.subreddit(subreddit_name)
        
        # Rate limiting
        print("[+] Rate limit iÃ§in 5 saniye bekleniyor...")
        time.sleep(5)
        
        # PRAW video upload
        submission = subreddit.submit_video(
            title=title,
            video_path=media_path,
            videogif=False,
            without_websockets=True,
            nsfw=False,
            spoiler=False,
            send_replies=True,
            resubmit=True,
            timeout=300,
            thumbnail_path=None
        )
        
        if submission:
            print(f"[+] PRAW video yÃ¼klemesi baÅŸarÄ±lÄ± - ID: {submission.id}")
            print(f"[+] URL: https://reddit.com/r/{subreddit_name}/comments/{submission.id}")
            # Flair uygula
            try:
                if flair_id:
                    efid = _resolve_flair_id_for_subreddit(subreddit_name, flair_id, _FLAIR_NAME_BY_ID.get(flair_id))
                    if efid:
                        submission.flair.select(efid)
                        print(f"[+] Video flair uygulandÄ± (PRAW): {efid}")
            except Exception as fe:
                print(f"[UYARI] Video flair uygulanamadÄ± (PRAW): {fe}")
            # BaÅŸarÄ±lÄ±ysa Submission nesnesini dÃ¶ndÃ¼r
            return submission
        else:
            print("[HATA] PRAW submission oluÅŸturulamadÄ±")
            return False
            
    except Exception as praw_e:
        print(f"[HATA] PRAW fallback hatasÄ±: {praw_e}")
        return False

def try_alternative_upload(title, media_path, subreddit, flair_id=None):
    """WebSocket hatasÄ± durumunda alternatif yÃ¼kleme yÃ¶ntemleri"""
    
    print("[+] Alternatif yÃ¼kleme yÃ¶ntemleri deneniyor...")
    
    # 1. Daha kÃ¼Ã§Ã¼k boyutta yeniden kodlama
    try:
        print("[+] Video'yu daha kÃ¼Ã§Ã¼k boyutta yeniden kodluyorum...")
        
        alt_output = media_path.replace('.mp4', '_small.mp4')
        command = [
            "ffmpeg",
            "-i", media_path,
            "-c:v", "libx264",
            "-preset", "ultrafast",  # HÄ±zlÄ± kodlama
            "-crf", "28",  # Daha yÃ¼ksek sÄ±kÄ±ÅŸtÄ±rma
            "-vf", "scale=640:360",  # 360p'ye dÃ¼ÅŸÃ¼r
            "-c:a", "aac",
            "-b:a", "64k",  # Daha dÃ¼ÅŸÃ¼k ses kalitesi
            "-movflags", "+faststart",
            "-y",
            alt_output
        ]
        
        subprocess.run(command, check=True, capture_output=True, timeout=120)
        
        if os.path.exists(alt_output) and os.path.getsize(alt_output) > 0:
            print(f"[+] KÃ¼Ã§Ã¼k video oluÅŸturuldu: {os.path.getsize(alt_output)} bytes")
            
            # KÃ¼Ã§Ã¼k videoyu yÃ¼klemeyi dene - PRAW dokÃ¼mantasyonu uyumlu
            time.sleep(5)
            submission = subreddit.submit_video(
                title=title, 
                video_path=alt_output, 
                videogif=False,
                without_websockets=True,
                nsfw=False,
                spoiler=False,
                send_replies=True,
                resubmit=True,
                timeout=300,  # Daha uzun timeout
                thumbnail_path=None
            )
            
            if submission:
                print(f"[+] Alternatif yÃ¶ntemle video gÃ¶nderildi: {submission.url}")
                # Flair uygula
                try:
                    if flair_id:
                        submission.flair.select(flair_id)
                        print(f"[+] Video flair uygulandÄ± (alternatif): {flair_id}")
                except Exception as fe:
                    print(f"[UYARI] Video flair uygulanamadÄ± (alternatif): {fe}")
                
                # GeÃ§ici dosyalarÄ± temizle
                try:
                    os.remove(media_path)
                    os.remove(alt_output)
                    print("[+] GeÃ§ici dosyalar temizlendi")
                except:
                    pass
                    
                return True
            else:
                print("[HATA] Alternatif video yÃ¼kleme baÅŸarÄ±sÄ±z")
        else:
            print("[HATA] Alternatif video oluÅŸturulamadÄ±")
            
    except Exception as e:
        print(f"[HATA] Alternatif yÃ¼kleme baÅŸarÄ±sÄ±z: {e}")
    
    # 2. Son Ã§are text post kaldÄ±rÄ±ldÄ±: medya baÅŸarÄ±sÄ±z olursa burada dur
    print("[!] Alternatifler de baÅŸarÄ±sÄ±z oldu, text post fallback devre dÄ±ÅŸÄ±. Ä°ÅŸlem sonlandÄ±rÄ±lÄ±yor.")
    return False

def smart_split_title(text: str, max_len: int = 300):
    """Metni baÅŸlÄ±k ve kalan olarak akÄ±llÄ±ca ayÄ±rÄ±r.
    - max_len sÄ±nÄ±rÄ±nÄ± aÅŸmayacak ÅŸekilde son boÅŸlukta keser.
    - Kesme olduysa baÅŸlÄ±ÄŸa "â€¦" ekler ve kalan kÄ±smÄ± dÃ¶ndÃ¼rÃ¼r.
    """
    text = (text or "").strip()
    if len(text) <= max_len:
        return text, ""
    # Son boÅŸluÄŸu bul (maks uzunluk iÃ§inde)
    cutoff = text.rfind(" ", 0, max_len)
    if cutoff == -1:
        cutoff = max_len
    title = text[:cutoff].rstrip()
    remainder = text[cutoff:].lstrip()
    # BaÅŸlÄ±ÄŸa ellipsis ekle
    if title and not title.endswith("â€¦"):
        title = (title + " â€¦")[:max_len]
    return title, remainder


def submit_post(title, media_files, original_tweet_text="", remainder_text: str = ""):
    """GeliÅŸtirilmiÅŸ post gÃ¶nderme fonksiyonu - AI flair seÃ§imi ile"""
    subreddit = reddit.subreddit(SUBREDDIT)
    
    # Medya dosyalarÄ±nÄ± tÃ¼rlerine gÃ¶re ayÄ±r
    image_files = []
    video_files = []
    
    for media_file in media_files:
        if media_file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif')):
            image_files.append(media_file)
        elif media_file.lower().endswith('.mp4'):
            video_files.append(media_file)
    
    print(f"[+] Medya analizi: {len(image_files)} resim, {len(video_files)} video")

    # AI ile flair seÃ§imi (gallery dahil tÃ¼m yollar iÃ§in Ã¶nce seÃ§)
    has_video = len(video_files) > 0
    selected_flair_id = select_flair_with_ai(title, original_tweet_text, has_video=has_video)
    print(f"[+] SeÃ§ilen flair ID: {selected_flair_id}")
    # Resolve flair for this subreddit; if not valid, we'll omit flair during submit
    effective_flair_id = _resolve_flair_id_for_subreddit(SUBREDDIT, selected_flair_id, _FLAIR_NAME_BY_ID.get(selected_flair_id)) if selected_flair_id else None
    if not effective_flair_id and selected_flair_id:
        print(f"[UYARI] SeÃ§ilen flair bu subredditte yok. Flair'siz gÃ¶nderilecek. (ID: {selected_flair_id})")
    
    # Ã–nce resimleri gallery olarak yÃ¼kle (eÄŸer birden fazla resim varsa)
    if len(image_files) > 1:
        print(f"[+] {len(image_files)} resim gallery olarak yÃ¼kleniyor...")
        gallery_result = upload_gallery_via_redditwarp(title, image_files, SUBREDDIT, flair_id=selected_flair_id)
        if gallery_result:
            print("[+] Gallery baÅŸarÄ±yla yÃ¼klendi")
            # GeÃ§ici resim dosyalarÄ±nÄ± temizle
            for img_path in image_files:
                try:
                    if os.path.exists(img_path):
                        os.remove(img_path)
                        print(f"[+] GeÃ§ici resim silindi: {img_path}")
                except Exception as cleanup_e:
                    print(f"[UYARI] Resim silinirken hata: {cleanup_e}")
            # Gallery iÃ§in baÅŸlÄ±ÄŸa gÃ¶re ÅŸaka yorumunu dene
            _maybe_post_joke_comment(None, title, SUBREDDIT, original_tweet_text)
            return True
        else:
            # Fallback Ã¶ncesi, Reddit'te gÃ¶nderi oluÅŸmuÅŸ mÄ± kontrol et
            try:
                sr_obj = reddit.subreddit(SUBREDDIT)
                for s in sr_obj.new(limit=10):
                    author_name = getattr(s.author, 'name', '') or ''
                    if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                        print(f"[+] Gallery aslÄ±nda oluÅŸturulmuÅŸ (fallback iptal): {s.url}")
                        return True
            except Exception as ve:
                print(f"[UYARI] Fallback Ã¶ncesi doÄŸrulama hatasÄ±: {ve}")
            print("[!] Gallery yÃ¼klenemedi, tekil resim yÃ¼kleme deneniyor...")
    
    # Tekil resim veya video yÃ¼kleme (mevcut kod)
    
    # BaÅŸlÄ±k doÄŸrulama ve yedekler (Reddit 'Post title is required' hatasÄ±nÄ± Ã¶nlemek iÃ§in)
    raw_title = (title or "").strip()
    if not raw_title:
        # Yedek: orijinal tweet metni veya sabit baÅŸlÄ±k
        fallback = (original_tweet_text or "").strip()
        if fallback:
            raw_title = fallback
        else:
            raw_title = "Twitter medyasÄ±"
    # Reddit baÅŸlÄ±k limiti ~300 karakter
    title = raw_title[:300]
    
    if not media_files:
        # Medya yoksa sadece text post
        try:
            print("[+] Medya yok, text post gÃ¶nderiliyor.")
            if effective_flair_id:
                submission = subreddit.submit(title=title, selftext=(remainder_text or ""), flair_id=effective_flair_id)
            else:
                submission = subreddit.submit(title=title, selftext=(remainder_text or ""))
            print(f"[+] Text post gÃ¶nderildi: {submission.url}")
            # Tweet sahibinin ÅŸaka notunu yorum olarak ekle
            _maybe_post_joke_comment(submission, title, SUBREDDIT, original_tweet_text)
            return True
        except Exception as e:
            print(f"[HATA] Text post hatasÄ±: {e}")
            return False
    
    # Medya var
    media_path = media_files[0]
    
    if not os.path.exists(media_path):
        print(f"[HATA] Medya dosyasÄ± bulunamadÄ±: {media_path}")
        return False
        
    file_size = os.path.getsize(media_path)
    if file_size == 0:
        print(f"[HATA] Medya dosyasÄ± boÅŸ: {media_path}")
        return False
        
    print(f"[+] Medya dosyasÄ± hazÄ±r: {media_path} ({file_size} bytes)")
    
    ext = os.path.splitext(media_path)[1].lower()
    
    try:
        if ext in [".jpg", ".jpeg", ".png", ".gif"]:
            # Resim yÃ¼kleme
            print(f"[+] Resim gÃ¶nderiliyor: {media_path}")
            if effective_flair_id:
                submission = subreddit.submit_image(title=title, image_path=media_path, flair_id=effective_flair_id)
            else:
                submission = subreddit.submit_image(title=title, image_path=media_path)
            print(f"[+] Resim baÅŸarÄ±yla gÃ¶nderildi: {submission.url}")
            # Uzun metnin kalanÄ± yorum olarak ekle
            try:
                if remainder_text:
                    submission.reply(remainder_text)
                    print("[+] BaÅŸlÄ±ÄŸÄ±n kalan kÄ±smÄ± yorum olarak eklendi")
            except Exception as ce:
                print(f"[UYARI] Kalan metin yorum olarak eklenemedi: {ce}")
            # Åžaka notunu ekle
            _maybe_post_joke_comment(submission, title, SUBREDDIT, original_tweet_text)
            return True
            
        elif ext in [".mp4", ".mov", ".webm"]:
            # Video yÃ¼kleme
            print(f"[+] Video gÃ¶nderiliyor: {media_path}")
            
            # Ã–nce dosya boyutunu kontrol et (Reddit iÃ§in gÃ¼venli limit)
            max_video_size = 512 * 1024 * 1024  # 512MB
            if file_size > max_video_size:
                print(f"[HATA] Video Ã§ok bÃ¼yÃ¼k ({file_size} bytes). Limit: {max_video_size} bytes")
                # Text post fallback kaldÄ±rÄ±ldÄ±
                return False
            
            # Video upload denemesi
            result = upload_video_via_reddit_api(title, media_path, SUBREDDIT, selected_flair_id)
            
            if result:
                print("[+] Video baÅŸarÄ±yla yÃ¼klendi!")
                # EÄŸer Submission nesnesi geldiyse kalan metni yorum olarak ekle
                try:
                    if hasattr(result, "reply") and remainder_text:
                        result.reply(remainder_text)
                        print("[+] BaÅŸlÄ±ÄŸÄ±n kalan kÄ±smÄ± video gÃ¶nderisine yorum olarak eklendi")
                    elif isinstance(result, str) and remainder_text:
                        # Deterministik: RedditWarp ID dÃ¶ndÃ¼
                        try:
                            praw_sub = reddit.submission(id=result)
                            praw_sub.reply(remainder_text)
                            print("[+] BaÅŸlÄ±ÄŸÄ±n kalan kÄ±smÄ± (ID ile) video gÃ¶nderisine yorum olarak eklendi")
                        except Exception as idc_e:
                            print(f"[UYARI] ID ile yorum ekleme baÅŸarÄ±sÄ±z: {idc_e}")
                    elif remainder_text:
                        # RedditWarp yolu: Submission nesnesi yok. Son gÃ¶nderiyi bulup yorum eklemeyi dene.
                        try:
                            for s in subreddit.new(limit=10):
                                author_name = getattr(s.author, 'name', '') or ''
                                if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                                    s.reply(remainder_text)
                                    print("[+] BaÅŸlÄ±ÄŸÄ±n kalan kÄ±smÄ± (RedditWarp) video gÃ¶nderisine yorum olarak eklendi")
                                    break
                        except Exception as se:
                            print(f"[UYARI] RedditWarp sonrasÄ± yorum ekleme baÅŸarÄ±sÄ±z: {se}")
                except Exception as ve:
                    print(f"[UYARI] Video yorum ekleme baÅŸarÄ±sÄ±z: {ve}")
                # Åžaka notunu ekle
                try:
                    if hasattr(result, "reply"):
                        _maybe_post_joke_comment(result, title, SUBREDDIT, original_tweet_text)
                    elif isinstance(result, str):
                        _maybe_post_joke_comment(result, title, SUBREDDIT, original_tweet_text)
                    else:
                        _maybe_post_joke_comment(None, title, SUBREDDIT, original_tweet_text)
                except Exception as _je:
                    print(f"[UYARI] Åžaka notu eklenemedi (video): {_je}")
                return True
            else:
                print("[!] Video yÃ¼klenemedi, alternatif yÃ¶ntemler deneniyor...")
                # Alternatif yÃ¶ntem dene
                alt_success = try_alternative_upload(title, media_path, subreddit)
                if alt_success:
                    # Alternatif yol ile gÃ¶nderildiyse baÅŸlÄ±k bazlÄ± ÅŸaka yorumunu dene
                    _maybe_post_joke_comment(None, title, SUBREDDIT, original_tweet_text)
                    return True
                else:
                    # Son Ã§are text post kaldÄ±rÄ±ldÄ±
                    return False
                
        else:
            print(f"[!] Desteklenmeyen dosya tÃ¼rÃ¼: {ext}")
            if effective_flair_id:
                submission = subreddit.submit(title=title, selftext=(remainder_text or ""), flair_id=effective_flair_id)
            else:
                submission = subreddit.submit(title=title, selftext=(remainder_text or ""))
            print(f"[+] Text post gÃ¶nderildi: {submission.url}")
            _maybe_post_joke_comment(submission, title, SUBREDDIT, original_tweet_text)
            return True
            
    except Exception as e:
        print(f"[HATA] Post gÃ¶nderimi baÅŸarÄ±sÄ±z: {e}")
        
        # Ã–zel durum: Reddit PRAW 'Websocket error. Check your media file. Your post may still have been created.'
        # Bu hata geldiÄŸinde ve medya varsa ASLA text post gÃ¶nderme.
        try:
            err_msg = str(e).lower()
        except Exception:
            err_msg = ""

        if (media_files and len(media_files) > 0) and ("websocket error" in err_msg and "check your media file" in err_msg):
            print("[!] Websocket error sonrasÄ± text post atlanÄ±yor (medya var). Reddit'te gÃ¶nderi oluÅŸmuÅŸ olabilir, kontrol ediliyor...")
            # Son 10 gÃ¶nderide baÅŸlÄ±ÄŸa bakarak oluÅŸmuÅŸ mu kontrol et
            try:
                for s in subreddit.new(limit=10):
                    author_name = getattr(s.author, 'name', '') or ''
                    if author_name.lower() == (REDDIT_USERNAME or '').lower() and s.title == title:
                        print(f"[+] GÃ¶nderi aslÄ±nda oluÅŸturulmuÅŸ: {s.url}")
                        return True
            except Exception as chk_e:
                print(f"[UYARI] Hata sonrasÄ± doÄŸrulama yapÄ±lamadÄ±: {chk_e}")
            # Emin olamÄ±yorsak baÅŸarÄ±sÄ±z say ve tekrar denemeye bÄ±rak
            return False

        # DiÄŸer hatalarda text post fallback devre dÄ±ÅŸÄ±
        return False
    finally:
        # GeÃ§ici dosyalarÄ± temizle
        try:
            if "temp_media" in media_path and os.path.exists(media_path):
                os.remove(media_path)
                print(f"[+] GeÃ§ici dosya temizlendi: {media_path}")
        except:
            pass

def load_posted_tweet_ids():
    """ðŸ§¹ Daha Ã¶nce gÃ¶nderilmiÅŸ tweet ID'lerini veritabanÄ±ndan veya dosyadan yÃ¼kle (memory optimized)"""
    # 1) VeritabanÄ± kullanÄ±labiliyorsa DB'den oku
    if USE_DB_FOR_POSTED_IDS:
        try:
            _ensure_posted_ids_table()
            ids = _db_load_posted_ids()
            # ðŸ§¹ Logging azaltÄ±ldÄ±
            result = set(ids)
            # ðŸ§¹ Temizlik
            del ids
            return result
        except Exception as e:
            if FAIL_IF_DB_UNAVAILABLE:
                raise RuntimeError(f"DB gerekli ancak eriÅŸilemedi (load): {e}")
            # ðŸ§¹ Logging azaltÄ±ldÄ± - sadece hata durumunda
    
    # 2) Dosya fallback - ðŸ§¹ Generator kullan
    posted_ids_file = "posted_tweet_ids.txt"
    alt_posted_ids_file = "posted_tweets_ids.txt"  # legacy/mistyped filename support
    posted_ids = set()
    
    def _read_file_lines(filename):
        """ðŸ§¹ Generator - dosyayÄ± satÄ±r satÄ±r oku"""
        try:
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    for line in f:
                        tweet_id = line.strip()
                        if tweet_id:
                            yield tweet_id
        except Exception:
            pass
    
    try:
        # Ana dosyayÄ± oku
        for tweet_id in _read_file_lines(posted_ids_file):
            posted_ids.add(tweet_id)
        
        # Legacy dosyayÄ± oku
        for tweet_id in _read_file_lines(alt_posted_ids_file):
            posted_ids.add(tweet_id)
            
    except Exception:
        pass  # ðŸ§¹ Logging azaltÄ±ldÄ±
    
    return posted_ids

def save_posted_tweet_id(tweet_id):
    """Yeni gÃ¶nderilmiÅŸ tweet ID'sini veritabanÄ±na veya dosyaya kaydet"""
    # 1) VeritabanÄ± kullanÄ±labiliyorsa Ã¶nce DB'ye yaz
    if USE_DB_FOR_POSTED_IDS:
        try:
            _ensure_posted_ids_table()
            _db_save_posted_id(tweet_id)
            # Son 8 kaydÄ± tut, eskilerini sil
            try:
                _db_prune_posted_ids_keep_latest(POSTED_IDS_RETENTION)
            except Exception as _prune_e:
                print(f"[UYARI] (DB) Prune baÅŸarÄ±sÄ±z: {_prune_e}")
            print(f"[+] (DB) Tweet ID kaydedildi: {tweet_id}")
            return
        except Exception as e:
            if FAIL_IF_DB_UNAVAILABLE:
                raise RuntimeError(f"DB gerekli ancak eriÅŸilemedi (save): {e}")
            print(f"[UYARI] (DB) Tweet ID kaydedilemedi, dosyaya dÃ¼ÅŸÃ¼lecek: {e}")
    # 2) Dosya fallback
    posted_ids_file = "posted_tweet_ids.txt"
    try:
        with open(posted_ids_file, 'a', encoding='utf-8') as f:
            f.write(f"{tweet_id}\n")
        # Dosyada da son 8 kaydÄ± tut
        try:
            _file_prune_posted_ids_keep_latest(posted_ids_file, POSTED_IDS_RETENTION)
        except Exception as _fprune_e:
            print(f"[UYARI] (Fallback) Prune baÅŸarÄ±sÄ±z: {_fprune_e}")
        print(f"[+] (Fallback) Tweet ID dosyaya kaydedildi: {tweet_id}")
    except Exception as e:
        print(f"[UYARI] (Fallback) Tweet ID kaydedilirken hata: {e}")

############################################################
# Postgres helper functions for posted_tweet_ids persistence
############################################################
_POSTED_IDS_TABLE_SQL = (
    "CREATE TABLE IF NOT EXISTS posted_tweet_ids (\n"
    "    id BIGINT PRIMARY KEY,\n"
    "    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n"
    ")"
)

def _db_prune_posted_ids_keep_latest(limit: int = 8):
    """posted_tweet_ids tablosunda yalnÄ±zca en yeni 'limit' kadar kaydÄ± (created_at DESC) bÄ±rak."""
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    DELETE FROM posted_tweet_ids
                    WHERE id NOT IN (
                        SELECT id FROM posted_tweet_ids
                        ORDER BY created_at DESC, id DESC
                        LIMIT %s
                    )
                    """,
                    (limit,),
                )
    finally:
        conn.close()

def _file_prune_posted_ids_keep_latest(file_path: str = "posted_tweet_ids.txt", limit: int = 8):
    """Dosyada son 'limit' satÄ±rÄ± koru, eskileri sil."""
    try:
        if not os.path.exists(file_path):
            return
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [ln.strip() for ln in f if ln.strip()]
        if len(lines) <= limit:
            return
        to_keep = lines[-limit:]
        with open(file_path, 'w', encoding='utf-8') as f:
            for tid in to_keep:
                f.write(f"{tid}\n")
        print(f"[+] (Prune) {file_path} son {limit} ID ile gÃ¼ncellendi")
    except Exception as e:
        print(f"[UYARI] (Prune) Dosya temizleme hatasÄ±: {e}")

# -------------------- Repost (bf6_tr) storage helpers --------------------
_POSTED_RT_TABLE_SQL = (
    "CREATE TABLE IF NOT EXISTS posted_retweet_ids (\n"
    "    id BIGINT PRIMARY KEY,\n"
    "    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n"
    ")"
)

def _ensure_posted_retweet_ids_table():
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute(_POSTED_RT_TABLE_SQL)
    finally:
        conn.close()

def _db_save_posted_retweet_id(tweet_id: str):
    try:
        id_int = int(str(tweet_id))
    except Exception:
        return
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO posted_retweet_ids (id) VALUES (%s) ON CONFLICT (id) DO NOTHING",
                    (id_int,),
                )
    finally:
        conn.close()

def _db_prune_posted_retweets_keep_latest(limit: int = 3):
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    DELETE FROM posted_retweet_ids
                    WHERE id NOT IN (
                        SELECT id FROM posted_retweet_ids
                        ORDER BY created_at DESC, id DESC
                        LIMIT %s
                    )
                    """,
                    (limit,),
                )
    finally:
        conn.close()

def _file_prune_posted_retweets_keep_latest(file_path: str = "posted_retweet_ids.txt", limit: int = 3):
    try:
        if not os.path.exists(file_path):
            return
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = [ln.strip() for ln in f if ln.strip()]
        if len(lines) <= limit:
            return
        to_keep = lines[-limit:]
        with open(file_path, 'w', encoding='utf-8') as f:
            for tid in to_keep:
                f.write(f"{tid}\n")
        print(f"[+] (Prune) {file_path} son {limit} RT ID ile gÃ¼ncellendi")
    except Exception as e:
        print(f"[UYARI] (Prune) RT dosya temizleme hatasÄ±: {e}")

def save_posted_retweet_id(tweet_id):
    """bf6_tr repost ID'sini kalÄ±cÄ± depoya yaz ve sadece son 3 kaydÄ± tut."""
    if USE_DB_FOR_POSTED_IDS:
        try:
            _ensure_posted_retweet_ids_table()
            _db_save_posted_retweet_id(tweet_id)
            try:
                _db_prune_posted_retweets_keep_latest(3)
            except Exception as _rt_prune_e:
                print(f"[UYARI] (DB) RT prune baÅŸarÄ±sÄ±z: {_rt_prune_e}")
            print(f"[+] (DB) RT ID kaydedildi: {tweet_id}")
            return
        except Exception as e:
            if FAIL_IF_DB_UNAVAILABLE:
                raise RuntimeError(f"DB gerekli ancak eriÅŸilemedi (save RT): {e}")
            print(f"[UYARI] (DB) RT ID kaydedilemedi, dosyaya dÃ¼ÅŸÃ¼lecek: {e}")
    # Fallback file mode
    rt_file = "posted_retweet_ids.txt"
    try:
        with open(rt_file, 'a', encoding='utf-8') as f:
            f.write(f"{tweet_id}\n")
        try:
            _file_prune_posted_retweets_keep_latest(rt_file, 3)
        except Exception as _frt_e:
            print(f"[UYARI] (Fallback) RT prune baÅŸarÄ±sÄ±z: {_frt_e}")
        print(f"[+] (Fallback) RT ID dosyaya kaydedildi: {tweet_id}")
    except Exception as e:
        print(f"[UYARI] (Fallback) RT ID kaydedilirken hata: {e}")

def _db_connect():
    """ðŸ§¹ Get a psycopg2 connection using DATABASE_URL (lazy import)"""
    # ðŸ§¹ Lazy import - sadece ihtiyaÃ§ duyulduÄŸunda import et
    try:
        import psycopg2
        from psycopg2.extras import RealDictCursor
    except ImportError:
        raise RuntimeError("psycopg2 mevcut deÄŸil")
    
    if not DATABASE_URL:
        raise RuntimeError("DATABASE_URL env yok")
    
    # Sanitize value: strip spaces/newlines
    dsn = (DATABASE_URL or "").strip()
    # Strip accidental leading key e.g. "DATABASE_URL=postgres://..."
    if dsn.lower().startswith("database_url="):
        dsn = dsn.split("=", 1)[1].strip()
    # Strip surrounding quotes if present
    if (dsn.startswith('"') and dsn.endswith('"')) or (dsn.startswith("'") and dsn.endswith("'")):
        dsn = dsn[1:-1]
    
    # ðŸ§¹ Logging azaltÄ±ldÄ± - debug log kaldÄ±rÄ±ldÄ±
    # Connect
    conn = psycopg2.connect(dsn)
    return conn

def _ensure_posted_ids_table():
    """Ensure table exists."""
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute(_POSTED_IDS_TABLE_SQL)
    finally:
        conn.close()

def _db_load_posted_ids():
    """Load all posted ids from DB as list of strings."""
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id FROM posted_tweet_ids")
                rows = cur.fetchall()
                return [str(r[0]) for r in rows]
    finally:
        conn.close()

def _db_save_posted_id(tweet_id: str):
    """Insert id if not exists (id bigint)."""
    # Convert to int if possible; ignore if non-numeric
    try:
        id_int = int(str(tweet_id))
    except Exception:
        # store as bigint not possible; skip
        return
    conn = _db_connect()
    try:
        with conn:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO posted_tweet_ids (id) VALUES (%s) ON CONFLICT (id) DO NOTHING",
                    (id_int,),
                )
    finally:
        conn.close()

def get_latest_tweets_with_retweet_check(count: int = 8):
    """ðŸ§¹ TWSCRAPE ile tweet Ã§ekme (memory optimized)
    Sadece TWSCRAPE kullanÄ±r, fallback'ler kaldÄ±rÄ±ldÄ±.
    DÃ¶nÃ¼ÅŸ: list[{'id','text','created_at','media_urls'}]
    """
    try:
        # Global rate-limit uygula
        global LAST_REQUEST_TIME
        current_time = time.time()
        time_since_last_request = current_time - LAST_REQUEST_TIME
        if time_since_last_request < MIN_REQUEST_INTERVAL:
            wait_time = MIN_REQUEST_INTERVAL - time_since_last_request
            time.sleep(wait_time)
        LAST_REQUEST_TIME = time.time()

        def _twscrape_fetch_sync():
            async def _run():
                api = None
                user = None
                tweets_generator = None
                try:
                    api = await init_twscrape_api()
                    # KullanÄ±cÄ±yÄ± ID ile getir (daha gÃ¼venilir)
                    user = await api.user_by_id(int(TWITTER_USER_ID))
                    if not user:
                        print(f"[HATA] Twitter kullanÄ±cÄ±sÄ± bulunamadÄ±: ID {TWITTER_USER_ID}")
                        return []
                    
                    # ðŸ§¹ Generator kullan - bÃ¼yÃ¼k listeleri RAM'e yÃ¼kleme
                    tweets_generator = api.user_tweets(user.id, limit=max(10, count * 3))
                    out = []
                    
                    async for tw in tweets_generator:
                        # Reply veya retweet olanlarÄ± atla
                        if getattr(tw, 'inReplyToTweetId', None):
                            continue
                        if getattr(tw, 'retweetedTweet', None):
                            continue
                        # AlÄ±ntÄ± (quote) tweet'leri atla
                        if getattr(tw, 'quotedTweet', None) or getattr(tw, 'isQuoted', False) or getattr(tw, 'isQuote', False):
                            continue

                        # Medya URL'lerini Ã§Ä±kar
                        media_urls = []
                        md = getattr(tw, 'media', None)
                        if md:
                            # ðŸ§¹ Lazy evaluation - sadece gerektiÄŸinde iÅŸle
                            photos = getattr(md, 'photos', []) or []
                            for p in photos:
                                url = getattr(p, 'url', None)
                                if url:
                                    media_urls.append(url)
                            
                            videos = getattr(md, 'videos', []) or []
                            for v in videos:
                                variants = getattr(v, 'variants', []) or []
                                if variants:
                                    best = max(variants, key=lambda x: getattr(x, 'bitrate', 0))
                                    url = getattr(best, 'url', None)
                                    if url:
                                        media_urls.append(url)
                            
                            animated = getattr(md, 'animated', []) or []
                            for a in animated:
                                url = getattr(a, 'videoUrl', None)
                                if url:
                                    media_urls.append(url)
                            
                            # ðŸ§¹ Temizlik - kullanÄ±lmayan objeleri serbest bÄ±rak
                            del photos, videos, animated

                        tweet_data = {
                            'id': str(getattr(tw, 'id', getattr(tw, 'id_str', ''))),
                            'text': getattr(tw, 'rawContent', ''),
                            'created_at': getattr(tw, 'date', None),
                            'media_urls': media_urls,
                            'url': getattr(tw, 'url', None),
                        }
                        out.append(tweet_data)
                        
                        # ðŸ§¹ Temizlik
                        del media_urls, md, tweet_data
                        
                        if len(out) >= count:
                            break
                    
                    # Eskiden yeniye sÄ±rala
                    try:
                        out.sort(key=_tweet_sort_key, reverse=False)
                    except Exception:
                        pass
                    
                    return out
                except Exception as e:
                    return []
                finally:
                    # ðŸ§¹ Temizlik - kullanÄ±lmayan objeleri serbest bÄ±rak
                    del api, user, tweets_generator

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(_run())
                return result
            finally:
                loop.close()
                # ðŸ§¹ Loop temizliÄŸi
                del loop

        # TWSCRAPE PRIMARY (fallback'ler kaldÄ±rÄ±ldÄ±)
        normalized = _twscrape_fetch_sync()
        
        # ðŸ§¹ Temizlik
        del _twscrape_fetch_sync
        
        return normalized or []
        
    except Exception as e:
        return []

def main_loop():
    # Persistent storage ile posted tweet IDs'leri yÃ¼kle
    posted_tweet_ids = load_posted_tweet_ids()
    # Compute high-watermark from persisted IDs
    try:
        _numeric_ids = [int(str(s)) for s in posted_tweet_ids if str(s).isdigit()]
        max_seen_id = max(_numeric_ids) if _numeric_ids else 0
    except Exception:
        max_seen_id = 0
    
    print(f"[+] Reddit Bot baÅŸlatÄ±lÄ±yor...")
    print(f"[+] Subreddit: r/{SUBREDDIT}")
    print(f"[+] Twitter: @{TWITTER_SCREENNAME} (ID: {TWITTER_USER_ID})")
    print("[+] Retweet'ler otomatik olarak atlanacak")
    print(f"[+] Åžu ana kadar {len(posted_tweet_ids)} tweet iÅŸlenmiÅŸ")
    
    # .env ile verilen Ã¶zel tweet ID'lerini (bir kereye mahsus) iÅŸle
    try:
        env_ids: list[str] = []
        if PROCESS_TWEET_ID:
            env_ids.append(PROCESS_TWEET_ID)
        if PROCESS_TWEET_IDS:
            env_ids.extend([s.strip() for s in PROCESS_TWEET_IDS.split(',') if s.strip()])
        # Dedupe while preserving order
        seen = set()
        env_ids = [x for x in env_ids if not (x in seen or seen.add(x))]
        for eid in env_ids:
            if eid in PROCESSED_ENV_IDS:
                continue
            if eid in posted_tweet_ids:
                print(f"[ENV] Tweet ID zaten iÅŸlenmiÅŸ gÃ¶rÃ¼nÃ¼yor, atlanÄ±yor: {eid}")
                PROCESSED_ENV_IDS.add(eid)
                continue
            print(f"[ENV] Ã–zel tweet ID iÅŸleniyor: {eid}")
            res = process_specific_tweet(eid)
            ok = isinstance(res, dict) and res.get("processed") is True
            reason = None if ok else (res.get("reason") if isinstance(res, dict) else "unknown")
            if ok:
                posted_tweet_ids.add(eid)
                save_posted_tweet_id(eid)
                print(f"[ENV] BaÅŸarÄ±lÄ±: {eid}")
            else:
                print(f"[ENV] Ä°ÅŸlenemedi: {eid} | sebep: {reason}")
            PROCESSED_ENV_IDS.add(eid)
    except Exception as _env_proc_err:
        print(f"[UYARI] .env tweet ID iÅŸleme hatasÄ±: {_env_proc_err}")
    
    while True:
        try:
            print("\n" + "="*50)
            print(f"[+] Tweet kontrol ediliyor... ({time.strftime('%Y-%m-%d %H:%M:%S')})")
            # Ã–nce planlÄ± haftalÄ±k gÃ¶nderiyi kontrol et/oluÅŸtur
            _create_and_pin_weekly_post_if_due()
            
            # Son 8 tweet'i al ve retweet kontrolÃ¼ yap (daha fazla tweet kontrol et)
            tweets_data = get_latest_tweets_with_retweet_check(8)
            
            if isinstance(tweets_data, dict) and "error" in tweets_data:
                print(f"[!] TWSCRAPE hatasÄ±: {tweets_data['error']}")
                time.sleep(MIN_REQUEST_INTERVAL)
                continue
            elif not tweets_data:
                print("[!] Tweet bulunamadÄ± veya TWSCRAPE hatasÄ±.")
                time.sleep(MIN_REQUEST_INTERVAL)
                continue
            
            tweets = tweets_data if isinstance(tweets_data, list) else tweets_data.get("tweets", [])
            if not tweets:
                print("[!] Ä°ÅŸlenecek tweet bulunamadÄ±.")
                time.sleep(MIN_REQUEST_INTERVAL)
                continue
            
            # Eskiden yeniye iÅŸle - created_at/id'e gÃ¶re deterministik sÄ±rala
            def _tweet_sort_key(td):
                # created_at zamanÄ± (varsa) -> epoch saniyesi
                ts = td.get('created_at') if isinstance(td, dict) else None
                tsv = 0.0
                try:
                    if hasattr(ts, 'timestamp'):
                        tsv = float(ts.timestamp())
                    elif isinstance(ts, (int, float)):
                        tsv = float(ts)
                except Exception:
                    tsv = 0.0
                # numeric tweet id (Snowflake) -> daha bÃ¼yÃ¼k daha yeni
                idv = 0
                if isinstance(td, dict):
                    for k in ('id', 'tweet_id', 'id_str', 'rest_id'):
                        v = td.get(k)
                        if v is None:
                            continue
                        s = str(v).strip()
                        if s.isdigit():
                            try:
                                idv = int(s)
                                break
                            except Exception:
                                pass
                return (tsv, idv)

            try:
                tweets.sort(key=_tweet_sort_key, reverse=False)
            except Exception:
                # Her ihtimale karÅŸÄ± Ã¶nce reverse deneyip sonra iÅŸle
                tweets = list(reversed(tweets))
            print(f"[+] {len(tweets)} tweet bulundu, eskiden yeniye doÄŸru iÅŸlenecek...")
            
            # Her tweet'i eskiden yeniye doÄŸru iÅŸle
            for tweet_index, tweet_data in enumerate(tweets, 1):
                # ID normalizasyonu
                tweet_id = (
                    (tweet_data.get("tweet_id") if isinstance(tweet_data, dict) else None)
                    or (tweet_data.get("id") if isinstance(tweet_data, dict) else None)
                    or (tweet_data.get("id_str") if isinstance(tweet_data, dict) else None)
                    or (tweet_data.get("rest_id") if isinstance(tweet_data, dict) else None)
                )
                if tweet_id is not None:
                    tweet_id = str(tweet_id).strip()
                if not tweet_id:
                    print(f"[HATA] Tweet {tweet_index}/{len(tweets)} - Tweet ID bulunamadÄ±!")
                    continue
                # Engelli tweet ID'lerini atla ve iÅŸlenmiÅŸ olarak kaydet
                if tweet_id in EXCLUDED_TWEET_IDS:
                    print(f"[SKIP] Engelli tweet ID (gÃ¶nderilmeyecek): {tweet_id}")
                    posted_tweet_ids.add(tweet_id)
                    save_posted_tweet_id(tweet_id)
                    # Update watermark if needed
                    try:
                        if HIGH_WATERMARK_ENABLED and str(tweet_id).isdigit():
                            ti = int(tweet_id)
                            if ti > max_seen_id:
                                max_seen_id = ti
                    except Exception:
                        pass
                    continue
                
                # High-watermark filter: skip anything older/equal than last seen
                if HIGH_WATERMARK_ENABLED and max_seen_id:
                    try:
                        if str(tweet_id).isdigit() and int(tweet_id) <= max_seen_id:
                            print(f"[SKIP] High-watermark nedeniyle atlandÄ± (<= {max_seen_id}): {tweet_id}")
                            continue
                    except Exception:
                        pass
                
                if tweet_id in posted_tweet_ids:
                    print(f"[!] Tweet {tweet_index}/{len(tweets)} zaten iÅŸlendi: {tweet_id}")
                    continue
                
                print(f"[+] Tweet {tweet_index}/{len(tweets)} iÅŸleniyor: {tweet_id}")
                posted_tweet_ids.add(tweet_id)
                save_posted_tweet_id(tweet_id)
                print(f"[+] Tweet ID kaydedildi (iÅŸlem Ã¶ncesi): {tweet_id}")
                print(f"[+] Tweet linki: https://x.com/{TWITTER_SCREENNAME}/status/{tweet_id}")
                # Update watermark after save
                try:
                    if HIGH_WATERMARK_ENABLED and str(tweet_id).isdigit():
                        ti = int(tweet_id)
                        if ti > max_seen_id:
                            max_seen_id = ti
                except Exception:
                    pass
                
                # Tweet metni ve Ã§eviri
                text = tweet_data.get("text", "")
                print(f"[+] Orijinal Tweet: {text[:100]}{'...' if len(text) > 100 else ''}")
                cleaned_text = clean_tweet_text(text)
                print(f"[+] TemizlenmiÅŸ Tweet: {cleaned_text[:100]}{'...' if len(cleaned_text) > 100 else ''}")
                # Countdown filter: skip if pattern like 'XX days until' and XX > 10
                try:
                    cd_days = _extract_countdown_days(text) or _extract_countdown_days(cleaned_text)
                except Exception:
                    cd_days = None
                if cd_days is not None and cd_days > 10:
                    print(f"[SKIP] Geri sayÄ±m ({cd_days} gÃ¼n) > 10: {tweet_id}")
                    continue
                
                # Medya Ã§Ä±karÄ±mÄ± (Ã¶nce video var mÄ± tespit et)
                print("[+] Medya URL'leri Ã§Ä±karÄ±lÄ±yor...")
                media_urls = get_media_urls_from_tweet_data(tweet_data)
                media_files = []
                
                image_urls: list[str] = []
                video_urls: list[str] = []
                for media_url in media_urls:
                    u = media_url.lower()
                    is_image = (
                        any(ext in u for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']) or
                        'format=jpg' in u or 'format=jpeg' in u or 'format=png' in u or 'format=webp' in u or
                        'pbs.twimg.com/media' in u
                    )
                    is_video = ('.mp4' in u or 'format=mp4' in u or 'video.twimg.com' in u)
                    if is_image:
                        image_urls.append(media_url)
                    elif is_video:
                        video_urls.append(media_url)
                print(f"[+] Medya analizi: {len(image_urls)} resim, {len(video_urls)} video")

                # Medya baÄŸlamÄ±yla Ã§eviri (reload->ÅžarjÃ¶r vb.)
                translated = translate_text(cleaned_text, has_video=bool(video_urls))
                if translated:
                    print(f"[+] Ã‡eviri: {translated[:100]}{'...' if len(translated) > 100 else ''}")
                else:
                    print(f"[UYARI] Ã‡eviri baÅŸarÄ±sÄ±z, tweet atlanÄ±yor: {tweet_id}")
                    continue
                
                # Resimler
                if len(image_urls) > 1:
                    print("[+] Birden fazla resim tespit edildi, toplu indirme baÅŸlatÄ±lÄ±yor...")
                    downloaded_images = download_multiple_images(image_urls, tweet_id)
                    media_files.extend(downloaded_images)
                elif len(image_urls) == 1:
                    media_url = image_urls[0]
                    ext = os.path.splitext(media_url)[1].split("?")[0] or ".jpg"
                    filename = f"temp_image_{tweet_id}_0{ext}"
                    print(f"[+] Tek resim indiriliyor: {media_url[:50]}...")
                    path = download_media(media_url, filename)
                    if path:
                        media_files.append(path)
                        print(f"[+] Resim hazÄ±r: {path}")
                
                # Videolar (twscrape Ã¼zerinden en kaliteli varyant)
                if video_urls:
                    try:
                        filename = f"temp_video_{tweet_id}_0.mp4"
                        print("[+] En kaliteli video indiriliyor (twscrape/HLS Ã¶ncelikli)...")
                        path = download_best_video_for_tweet(tweet_id, filename)
                        if path:
                            # SÃ¼re kontrolÃ¼: Reddit limitini aÅŸarsa tÃ¼m tweet'i atla
                            dur = get_video_duration_seconds(path)
                            if dur is not None and dur > REDDIT_MAX_VIDEO_SECONDS:
                                print(f"[SKIP] Video Ã§ok uzun ({dur:.1f}s > {REDDIT_MAX_VIDEO_SECONDS}s). Tweet atlanÄ±yor: {tweet_id}")
                                try:
                                    if os.path.exists(path):
                                        os.remove(path)
                                except Exception:
                                    pass
                                # Ã–nceden indirilen medya (resimler) varsa temizle
                                for fpath in media_files:
                                    try:
                                        if os.path.exists(fpath):
                                            os.remove(fpath)
                                    except Exception:
                                        pass
                                # Sonraki tweet'e geÃ§
                                continue

                            converted = f"converted_{filename}"
                            print(f"[+] Video dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor: {path} -> {converted}")
                            converted_path = convert_video_to_reddit_format(path, converted)
                            if converted_path:
                                media_files.append(converted_path)
                                print(f"[+] Video dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±lÄ±: {converted_path}")
                            else:
                                print("[!] Video dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±sÄ±z")
                            if os.path.exists(path):
                                os.remove(path)
                        else:
                            print("[!] Video indirilemedi (best-quality yoluyla)")
                    except Exception as media_e:
                        print(f"[HATA] Video iÅŸleme hatasÄ±: {media_e}")
                
                print(f"[+] Toplam {len(media_files)} medya dosyasÄ± hazÄ±r")
                
                # Medya doÄŸrulamasÄ±
                original_text = tweet_data.get("text", "")
                has_media_in_original = any(ind in original_text for ind in ['pic.twitter.com', 'video.twitter.com', 'pbs.twimg.com'])
                if has_media_in_original and len(media_files) == 0:
                    print(f"[UYARI] Orijinal tweet'te medya var ama indirilemedi, post atlanÄ±yor: {tweet_id}")
                    for fpath in media_files:
                        try:
                            if os.path.exists(fpath):
                                os.remove(fpath)
                        except Exception:
                            pass
                    continue
                
                # Post gÃ¶nderme
                candidates = [
                    (translated or "").strip(),
                    (cleaned_text or "").strip(),
                    (text or "").strip(),
                ]
                chosen_text = next((c for c in candidates if c), "")
                if not chosen_text:
                    chosen_text = f"@{TWITTER_SCREENNAME} paylaÅŸÄ±mÄ± - {tweet_id}"
                title_to_use, remainder_to_post = smart_split_title(chosen_text, 300)
                print(f"[+] KullanÄ±lacak baÅŸlÄ±k ({len(title_to_use)}): {title_to_use[:80]}{'...' if len(title_to_use) > 80 else ''}")
                if remainder_to_post:
                    print(f"[+] BaÅŸlÄ±ÄŸÄ±n kalan kÄ±smÄ± ({len(remainder_to_post)} karakter) gÃ¶nderi aÃ§Ä±klamasÄ±/yorum olarak eklenecek")
                print("[+] Reddit'e post gÃ¶nderiliyor...")
                success = submit_post(title_to_use, media_files, text, remainder_text=remainder_to_post)
                if success:
                    print(f"[+] Tweet baÅŸarÄ±yla iÅŸlendi: {tweet_id}")
                else:
                    print(f"[UYARI] Tweet iÅŸlenemedi ama ID zaten kaydedildi: {tweet_id}")
                    for fpath in media_files:
                        try:
                            if os.path.exists(fpath):
                                os.remove(fpath)
                                print(f"[+] GeÃ§ici dosya silindi: {fpath}")
                        except Exception as cleanup_e:
                            print(f"[UYARI] Dosya silinirken hata: {cleanup_e}")
                
                # Tweet'ler arasÄ± 5 dakika bekle (son tweet hariÃ§)
                if tweet_index < len(tweets):
                    print(f"[+] Sonraki tweet iÃ§in 5 dakika bekleniyor... ({tweet_index}/{len(tweets)} tamamlandÄ±)")
                    time.sleep(300)
            # EK: TheBFWire akÄ±ÅŸÄ±ndaki @bf6_tr retweet'lerini (8/8 tamamlandÄ±ysa) aynÄ± mantÄ±kla iÅŸle
            try:
                if isinstance(tweets, list) and len(tweets) >= 8:
                    print("\n" + "-"*50)
                    print("[+] Ek gÃ¶rev: @bf6_tr retweet'leri iÅŸleniyor (aynÄ± pipeline)...")
                    rt_list = get_latest_bf6_retweets(3)
                    if not rt_list:
                        print("[INFO] Ä°ÅŸlenecek @bf6_tr retweet'i bulunamadÄ±")
                    else:
                        print(f"[+] {len(rt_list)} retweet bulundu (eskiden yeniye iÅŸlenecek)")
                        for rt_index, tweet_data in enumerate(rt_list, 1):
                            tweet_id = (
                                (tweet_data.get("tweet_id") if isinstance(tweet_data, dict) else None)
                                or (tweet_data.get("id") if isinstance(tweet_data, dict) else None)
                                or (tweet_data.get("id_str") if isinstance(tweet_data, dict) else None)
                                or (tweet_data.get("rest_id") if isinstance(tweet_data, dict) else None)
                            )
                            if tweet_id is not None:
                                tweet_id = str(tweet_id).strip()
                            if not tweet_id:
                                print(f"[HATA] RT {rt_index}/{len(rt_list)} - Tweet ID bulunamadÄ±!")
                                continue
                            if tweet_id in EXCLUDED_TWEET_IDS:
                                print(f"[SKIP] Engelli tweet ID (RT): {tweet_id}")
                                posted_tweet_ids.add(tweet_id)
                                save_posted_tweet_id(tweet_id)
                                # AyrÄ± repost storage'Ä±na da yaz ve 3'e prune et
                                save_posted_retweet_id(tweet_id)
                                continue
                            if tweet_id in posted_tweet_ids:
                                print(f"[!] RT {rt_index}/{len(rt_list)} zaten iÅŸlendi: {tweet_id}")
                                continue
                            print(f"[+] RT {rt_index}/{len(rt_list)} iÅŸleniyor: {tweet_id}")
                            posted_tweet_ids.add(tweet_id)
                            save_posted_tweet_id(tweet_id)
                            # AyrÄ± repost storage'Ä±na da yaz ve 3'e prune et
                            save_posted_retweet_id(tweet_id)
                            print(f"[+] RT ID kaydedildi (iÅŸlem Ã¶ncesi): {tweet_id}")
                            print(f"[+] RT linki: https://x.com/{TWITTER_SCREENNAME}/status/{tweet_id}")

                            text = tweet_data.get("text", "")
                            print(f"[+] Orijinal RT Metin: {text[:100]}{'...' if len(text) > 100 else ''}")
                            cleaned_text = clean_tweet_text(text)
                            print(f"[+] TemizlenmiÅŸ RT Metin: {cleaned_text[:100]}{'...' if len(cleaned_text) > 100 else ''}")
                            # Countdown filter for RT: skip if 'XX days until' and XX > 10
                            try:
                                cd_days = _extract_countdown_days(text) or _extract_countdown_days(cleaned_text)
                            except Exception:
                                cd_days = None
                            if cd_days is not None and cd_days > 10:
                                print(f"[SKIP] RT Geri sayÄ±m ({cd_days} gÃ¼n) > 10: {tweet_id}")
                                continue
                            # 'Kaynak' BAÅžLIÄžI: yalnÄ±zca temizleme sonrasÄ± metin tamamen boÅŸsa
                            fallback_source_title = None
                            if not cleaned_text.strip():
                                rt_url = tweet_data.get("url") or f"https://x.com/i/web/status/{tweet_id}"
                                rt_author = extract_username_from_tweet_url(rt_url)
                                fallback_source_title = f"Kaynak: @{rt_author}"
                                translated = None
                                print(f"[INFO] RT temizlenince metin boÅŸ, baÅŸlÄ±k kaynak olarak ayarlanacak: {fallback_source_title}")
                            else:
                                # Ã–nce medya Ã§Ä±kar ve video var mÄ± tespit et
                                print("[+] RT Medya URL'leri Ã§Ä±karÄ±lÄ±yor...")
                                media_urls = get_media_urls_from_tweet_data(tweet_data)
                                media_files = []
                                image_urls = []
                                video_urls = []
                                for media_url in media_urls:
                                    u = media_url.lower()
                                    is_image = (
                                        any(ext in u for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']) or
                                        'format=jpg' in u or 'format=jpeg' in u or 'format=png' in u or 'format=webp' in u or
                                        'pbs.twimg.com/media' in u
                                    )
                                    is_video = ('.mp4' in u or 'format=mp4' in u or 'video.twimg.com' in u)
                                    if is_image:
                                        image_urls.append(media_url)
                                    elif is_video:
                                        video_urls.append(media_url)
                                print(f"[+] RT Medya analizi: {len(image_urls)} resim, {len(video_urls)} video")
                                translated = translate_text(cleaned_text, has_video=bool(video_urls))
                            if translated:
                                print(f"[+] RT Ã‡eviri: {translated[:100]}{'...' if len(translated) > 100 else ''}")
                            elif fallback_source_title:
                                # Ã‡eviri yok ama kaynak baÅŸlÄ±ÄŸÄ± mevcut; devam edilecek
                                print("[INFO] RT Ã§eviri atlandÄ±, kaynak baÅŸlÄ±ÄŸÄ± kullanÄ±lacak")
                            else:
                                print(f"[UYARI] RT Ã§eviri baÅŸarÄ±sÄ±z, atlanÄ±yor: {tweet_id}")
                                continue

                            if len(image_urls) > 1:
                                print("[+] RT: Birden fazla resim tespit edildi, toplu indirme baÅŸlatÄ±lÄ±yor...")
                                downloaded_images = download_multiple_images(image_urls, tweet_id)
                                media_files.extend(downloaded_images)
                            elif len(image_urls) == 1:
                                media_url = image_urls[0]
                                ext = os.path.splitext(media_url)[1].split("?")[0] or ".jpg"
                                filename = f"temp_image_{tweet_id}_0{ext}"
                                print(f"[+] RT Tek resim indiriliyor: {media_url[:50]}...")
                                path = download_media(media_url, filename)
                                if path:
                                    media_files.append(path)
                                    print(f"[+] RT Resim hazÄ±r: {path}")

                            if video_urls:
                                try:
                                    filename = f"temp_video_{tweet_id}_0.mp4"
                                    print("[+] RT En kaliteli video indiriliyor (twscrape/HLS Ã¶ncelikli)...")
                                    path = download_best_video_for_tweet(tweet_id, filename)
                                    if path:
                                        # SÃ¼re kontrolÃ¼: Reddit limitini aÅŸarsa tÃ¼m RT tweet'i atla
                                        dur = get_video_duration_seconds(path)
                                        if dur is not None and dur > REDDIT_MAX_VIDEO_SECONDS:
                                            print(f"[SKIP] RT Video Ã§ok uzun ({dur:.1f}s > {REDDIT_MAX_VIDEO_SECONDS}s). RT atlanÄ±yor: {tweet_id}")
                                            try:
                                                if os.path.exists(path):
                                                    os.remove(path)
                                            except Exception:
                                                pass
                                            for fpath in media_files:
                                                try:
                                                    if os.path.exists(fpath):
                                                        os.remove(fpath)
                                                except Exception:
                                                    pass
                                            # Bir sonraki RT'ye geÃ§
                                            continue

                                        converted = f"converted_{filename}"
                                        print(f"[+] RT Video dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor: {path} -> {converted}")
                                        converted_path = convert_video_to_reddit_format(path, converted)
                                        if converted_path:
                                            media_files.append(converted_path)
                                            print(f"[+] RT Video dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±lÄ±: {converted_path}")
                                        else:
                                            print("[!] RT Video dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±sÄ±z")
                                        if os.path.exists(path):
                                            os.remove(path)
                                    else:
                                        print("[!] RT Video indirilemedi (best-quality yoluyla)")
                                except Exception as media_e:
                                    print(f"[HATA] RT Video iÅŸleme hatasÄ±: {media_e}")

                            print(f"[+] RT Toplam {len(media_files)} medya dosyasÄ± hazÄ±r")

                            original_text = tweet_data.get("text", "")
                            has_media_in_original = any(ind in original_text for ind in ['pic.twitter.com', 'video.twitter.com', 'pbs.twimg.com'])
                            if has_media_in_original and len(media_files) == 0:
                                print(f"[UYARI] RT tweet'te medya var ama indirilemedi, atlanÄ±yor: {tweet_id}")
                                for fpath in media_files:
                                    try:
                                        if os.path.exists(fpath):
                                            os.remove(fpath)
                                    except Exception:
                                        pass
                                continue

                            # Raw 'text' KULLANMA: link sÄ±zmasÄ±nÄ± Ã¶nlemek iÃ§in sadece Ã§eviri veya temiz metin kullan
                            candidates = [
                                (translated or "").strip(),
                                (cleaned_text or "").strip(),
                            ]
                            chosen_text = next((c for c in candidates if c), "")
                            if not chosen_text:
                                # Sadece temizlenmiÅŸ metin boÅŸ olduÄŸunda oluÅŸturulan kaynak baÅŸlÄ±ÄŸÄ±nÄ± kullan
                                if 'fallback_source_title' in locals() and fallback_source_title:
                                    chosen_text = fallback_source_title
                                else:
                                    # Eski davranÄ±ÅŸ: genel fallback (Kaynak kullanma!)
                                    chosen_text = f"@{TWITTER_SCREENNAME} paylaÅŸÄ±mÄ± - {tweet_id}"
                            title_to_use, remainder_to_post = smart_split_title(chosen_text, 300)
                            print(f"[+] RT KullanÄ±lacak baÅŸlÄ±k ({len(title_to_use)}): {title_to_use[:80]}{'...' if len(title_to_use) > 80 else ''}")
                            if remainder_to_post:
                                print(f"[+] RT BaÅŸlÄ±ÄŸÄ±n kalan kÄ±smÄ± ({len(remainder_to_post)} karakter) gÃ¶nderi aÃ§Ä±klamasÄ±/yorum olarak eklenecek")
                            print("[+] RT Reddit'e post gÃ¶nderiliyor...")
                            success = submit_post(title_to_use, media_files, text, remainder_text=remainder_to_post)
                            if success:
                                print(f"[+] RT baÅŸarÄ±yla iÅŸlendi: {tweet_id}")
                            else:
                                print(f"[UYARI] RT iÅŸlenemedi ama ID zaten kaydedildi: {tweet_id}")
                                for fpath in media_files:
                                    try:
                                        if os.path.exists(fpath):
                                            os.remove(fpath)
                                            print(f"[+] RT GeÃ§ici dosya silindi: {fpath}")
                                    except Exception as cleanup_e:
                                        print(f"[UYARI] RT dosya silinirken hata: {cleanup_e}")
                            if rt_index < len(rt_list):
                                print(f"[+] Sonraki RT iÃ§in 5 dakika bekleniyor... ({rt_index}/{len(rt_list)} tamamlandÄ±)")
                                time.sleep(300)
            except Exception as _rt_err:
                print(f"[UYARI] @bf6_tr retweet iÅŸleme hatasÄ±: {_rt_err}")
        except Exception as loop_e:
            print(f"[HATA] Ana dÃ¶ngÃ¼ hatasÄ±: {loop_e}")
            import traceback
            traceback.print_exc()
        
        # DÄ±ÅŸ dÃ¶ngÃ¼ beklemesi: 5 dakika
        print(f"\n[+] Sonraki kontrol: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time() + 300))}")
        print("â³ 5 dakika bekleniyor...")
        time.sleep(300)

if __name__ == "__main__":
    # ðŸ§¹ Lazy import ve initialization
    try:
        import uvicorn
    except ImportError:
        print("[HATA] uvicorn mevcut deÄŸil")
        sys.exit(1)
    
    # FastAPI app'i initialize et
    app = _init_fastapi()
    
    # Lokal geliÅŸtirme/test iÃ§in HTTP sunucusunu ayaÄŸa kaldÄ±r
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port, reload=False)
